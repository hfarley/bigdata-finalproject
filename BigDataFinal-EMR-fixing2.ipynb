{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hfarley/bigdata-finalproject/blob/main/BigDataFinal-EMR-fixing2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "PWxBkqIkZA2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm spark-3.2.4-bin-hadoop3.2.tgz \n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "id": "X3azW0x7ZBjp",
        "outputId": "54442d09-79a9-46e4-864a-5f4f6be55334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 294140\n",
            "drwxr-xr-x  1 root root      4096 Apr 27 13:35 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "705ZGS6FZFGo",
        "outputId": "da357d9d-2dae-4d4c-aa03-5345ed893de2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIASUSGZLKF6GFOGX7D',\n",
        "    'aws_secret_access_key': 'EbMnumT5pb0i/RI9dcFeu2Qop44GQR48OlaLE+vg',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzELH//////////wEaDE2To2Qy9UnHP8pt7CLPAZfmxL8fG+k5cx0tmveiI6hOWfeoJ2dcEGwngHQgxWNjUEGSj5ZUpJjhDL3xr8U4cPMYC5ZsBUyfVKCrqsmVqafCJlKJ7/fWMDO9py0+I7/jKuOmywri0jJBKR9zqJo1Y+KqU1c9bJ3Pj5rDA10ZSabRgPsgwrBNXX+0Tdd0m78gbkqTdLdB5BkHCyGGCZhcX4S2i0NtVb0Dgq4q23nsuCmahEAqheDHv/5pjOkNKzlodr6/ilFmmMFaqyGM2IZEA3ToOYrksYxY+nIZgMUJ7iiMtr+iBjIt1UtAty41/9MCwFj07Q+qMsvgEtFC1Keb+uXc4uNeW6A5/EBR7J50xnfKiFnZ'\n",
        "}\n"
      ],
      "metadata": {
        "id": "wMJdCuDcNOGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLUSTER_ID = 'j-1FCUU6HNPWF75'\n",
        "\n",
        "\n",
        "def submit_job_1(app_name, pyfile_uri, month, year):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri,\n",
        "                     '--monthArg', str(month),\n",
        "                     '--yearArg', str(year)],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "amoQXhZFNYsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "metadata": {
        "id": "-W7ljr_sNOvf",
        "outputId": "0643039c-edc2-410b-e417-4c372fb6a1d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.26.123)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.123 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.29.123)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.123->boto3) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. query any month of any year within the database and it will show a map of the incidents during that month. \n",
        "# 2. input a road segment, and it will show a graph of the density of the times \n",
        "# that incidents occur in that road segment. \n",
        "# 3. query the average response time for each road segment, and sort it to find\n",
        "#  which road segments have the highest and lowest response times. \n",
        "\n"
      ],
      "metadata": {
        "id": "9oadSQOUZZ1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes: \n",
        "\n",
        "\n",
        "*   .show() will give the first 20 entries\n",
        "*   in the spark assignment we only used SparkContext, but here we also need Spark Session\n",
        "\n",
        "For the first task we need ID, latitude & longitude (or, alternatively geometry), and one of the time variables - I decided to use the local time variable since that would be more meaningful for someone trying to visualize the data\n",
        "\n",
        "We also add columns for month and year (based on time_local) for querying purposes\n",
        "\n",
        "* Need to decide how we want to visualize this and get the coordinates to draw a map of Davidson county which then shows the incidents\n",
        "* Month & year are specified as command line arguments\n",
        "\n"
      ],
      "metadata": {
        "id": "DWEy3kv2nSYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MONTH = 1\n",
        "YEAR = 2021"
      ],
      "metadata": {
        "id": "vrHIu8IjHsWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file month_incidents.py\n",
        "\n",
        "\n",
        "import argparse\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import year, month\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  parser = argparse.ArgumentParser(description='Process some strings.')\n",
        "  parser.add_argument('--monthArg', type=int, choices=range(1,13), help='month (1-12)', required=True)\n",
        "  parser.add_argument('--yearArg', type=int, choices=range(2017,2022), help='year (2017-2022)', required=True)\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  \n",
        "  try:\n",
        "\n",
        "    incidents = spark.read.parquet(\"s3://bigdatafinal-hannah-lucas/datasets/nfd_incidents_xd_seg.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"emdCardNumber\", \"time_utc\", \"response_time_sec\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"Dist_to_Seg\", \"XDSegID\")\n",
        "\n",
        "    incidents = incidents.withColumn(\"year\", year(incidents[\"time_local\"]))\n",
        "    incidents = incidents.withColumn(\"month\", month(incidents[\"time_local\"]))\n",
        "    \n",
        "    filtered_incidents = incidents.filter((incidents.year == args.yearArg) & (incidents.month == args.monthArg))\n",
        "    \n",
        "    filtered_incidents.show()\n",
        "\n",
        "    \n",
        "    filtered_incidents.write.parquet(\"s3://bigdatafinal-hannah-lucas/output/incidents_{month}_{year}.parquet\".format(month=args.monthArg, year=args.yearArg))\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTV-wNskZHt4",
        "outputId": "07da6965-1e2b-48cf-934e-7b1730567054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting month_incidents.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='month_incidents.py', Bucket='bigdatafinal-hannah-lucas', Key='month_incidents.py')"
      ],
      "metadata": {
        "id": "6lBbStddMmAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_job_1(app_name='month_incidents', pyfile_uri='s3://bigdatafinal-hannah-lucas/month_incidents.py', month=MONTH, year=YEAR)"
      ],
      "metadata": {
        "id": "Qh8z2eY8M0i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas pandas pyarrow\n",
        "!pip install s3fs"
      ],
      "metadata": {
        "id": "mDiy13AbE3zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import shapely\n",
        "import pyarrow.parquet as pq\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "bucket_name = \"bigdatafinal-hannah-lucas\"\n",
        "prefix = \"output/incidents_{month}_{year}.parquet\".format(month=MONTH, year=YEAR)\n",
        "suffix = '.parquet'\n",
        "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
        "for obj in response.get('Contents', []):\n",
        "    if obj['Key'].endswith(suffix):\n",
        "      key = obj['Key']\n",
        "\n",
        "# Read the Parquet file from S3\n",
        "parquet_file = s3.get_object(Bucket=bucket_name, Key=key)\n",
        "\n",
        "table = pq.read_table(BytesIO(parquet_file['Body'].read()))\n",
        "incidents = table.to_pandas()\n",
        "\n",
        "incidents['geometry']=incidents.geometry.apply(shapely.wkt.loads)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.scatter(incidents['longitude'], incidents['latitude'])\n",
        "ax.set_title('NFD Incidents')\n",
        "ax.set_xlabel('Longitude')\n",
        "ax.set_ylabel('Latitude')\n",
        "\n",
        "\n",
        "# Save the plot as jpg file\n",
        "buf = io.BytesIO()\n",
        "plt.savefig(buf, format='jpg')\n",
        "buf.seek(0)\n",
        "\n",
        "plot_name = 'month_incident_plots/{month}_{year}.jpg'.format(month=MONTH, year=YEAR)\n",
        "\n",
        "s3.upload_fileobj(buf, Bucket=bucket_name, Key=plot_name)"
      ],
      "metadata": {
        "id": "n_l1eNGUa3BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLUSTER_ID = 'j-1VO8T4QRWO0EU'\n",
        "\n",
        "\n",
        "def submit_job_2(app_name, pyfile_uri, XDSegID):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri,\n",
        "                     '--XDSegID', str(XDSegID)],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "ZgMlXpKufPq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XDSEGID = str(1524393684.0)"
      ],
      "metadata": {
        "id": "FUD6QeexqKe-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file road_density_incidents.py\n",
        "\n",
        "import argparse\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import hour\n",
        "import json\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser(description='Process some strings.')\n",
        "  parser.add_argument('--XDSegID', type=float, help='Enter desired XDSegID', required=True)\n",
        "  args = parser.parse_args()\n",
        "  \n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('road_density_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "  try:\n",
        "    # incidents = spark.read.parquet(\"s3://bigdatafinal-hannah-lucas/datasets/nfd_incidents_xd_seg.parquet\")\n",
        "    incidents = spark.read.parquet(\"incidents_10000.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"emdCardNumber\", \"time_utc\", \"latitude\", \"longitude\", \"response_time_sec\", \"geometry\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"Dist_to_Seg\")\n",
        "    incidents = incidents.withColumn(\"hour\", hour(incidents[\"time_local\"]))\n",
        "    ex_id = args.XDSegID\n",
        "    incidents = incidents.drop(\"time_local\")\n",
        "    incidents = incidents.filter(incidents.XDSegID == ex_id)\n",
        "    hours = incidents.select('hour').collect()\n",
        "    # hours = [val.hour for val in incidents.select('hour').collect()]\n",
        "    # filename = \"incidents_hourly_density_{XDSegID}.json\".format(XDSegID=int(args.XDSegID))\n",
        "    # hours.write.parquet(filename)\n",
        "    filename = \"s3://bigdatafinal-hannah-lucas/output/incidents_hourly_density_{XDSegID}.json\".format(XDSegID=int(args.XDSegID))\n",
        "    print(filename)\n",
        "    # with open(filename, \"w\") as handle:\n",
        "    #     x = json.dumps(hours, indent=4)\n",
        "    #     handle.write(x)\n",
        "    # s3object = s3.Object('bigdatafinal-hannah-lucas', filename)\n",
        "    # s3object.put(\n",
        "    #     Body=(bytes(json.dumps(hours).encode('UTF-8'))))\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "nlvh4p56ZrQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32411903-dbeb-4bd2-898f-16922ad9c275"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting road_density_incidents.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 road_density_incidents.py --XDSegID 1524393684.0"
      ],
      "metadata": {
        "id": "FM7XGvSg7IDY",
        "outputId": "d7faf079-23c7-4252-ea9e-5943f1a6bb30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e15c8263-7879-4f59-b9b2-cfb71fbeb3ef;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 977ms :: artifacts dl 23ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-e15c8263-7879-4f59-b9b2-cfb71fbeb3ef\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/14ms)\n",
            "23/05/01 16:42:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/05/01 16:42:32 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/05/01 16:42:32 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 16:42:32 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/05/01 16:42:32 INFO ResourceUtils: ==============================================================\n",
            "23/05/01 16:42:32 INFO SparkContext: Submitted application: road_density_incidents\n",
            "23/05/01 16:42:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/05/01 16:42:32 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/05/01 16:42:32 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/05/01 16:42:32 INFO SecurityManager: Changing view acls to: root\n",
            "23/05/01 16:42:32 INFO SecurityManager: Changing modify acls to: root\n",
            "23/05/01 16:42:32 INFO SecurityManager: Changing view acls groups to: \n",
            "23/05/01 16:42:32 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/05/01 16:42:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/05/01 16:42:33 INFO Utils: Successfully started service 'sparkDriver' on port 34415.\n",
            "23/05/01 16:42:33 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/05/01 16:42:33 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/05/01 16:42:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/05/01 16:42:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/05/01 16:42:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/05/01 16:42:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-06b111aa-4bb1-44a7-8502-0738832d0cb3\n",
            "23/05/01 16:42:33 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/05/01 16:42:33 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/05/01 16:42:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/05/01 16:42:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://e85709d55941:4040\n",
            "23/05/01 16:42:34 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://e85709d55941:34415/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://e85709d55941:34415/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://e85709d55941:34415/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://e85709d55941:34415/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://e85709d55941:34415/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://e85709d55941:34415/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://e85709d55941:34415/jars/com.101tec_zkclient-0.3.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://e85709d55941:34415/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://e85709d55941:34415/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://e85709d55941:34415/jars/log4j_log4j-1.2.17.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://e85709d55941:34415/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://e85709d55941:34415/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 16:42:34 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 16:42:34 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:34 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 16:42:35 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 16:42:35 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 16:42:35 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 16:42:35 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 16:42:35 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 16:42:35 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 16:42:35 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/log4j_log4j-1.2.17.jar\n",
            "23/05/01 16:42:35 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 16:42:35 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 16:42:35 INFO Executor: Starting executor ID driver on host e85709d55941\n",
            "23/05/01 16:42:35 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/log4j_log4j-1.2.17.jar\n",
            "23/05/01 16:42:35 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 16:42:35 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 16:42:35 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:35 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 16:42:36 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 16:42:36 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 16:42:36 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 16:42:36 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 16:42:36 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 16:42:36 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 16:42:36 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 16:42:36 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 16:42:36 INFO Executor: Fetching spark://e85709d55941:34415/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO TransportClientFactory: Successfully created connection to e85709d55941/172.28.0.12:34415 after 108 ms (0 ms spent in bootstraps)\n",
            "23/05/01 16:42:36 INFO Utils: Fetching spark://e85709d55941:34415/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp1401344178219687596.tmp\n",
            "23/05/01 16:42:36 INFO Utils: /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp1401344178219687596.tmp has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/05/01 16:42:36 INFO Executor: Adding file:/tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/05/01 16:42:36 INFO Executor: Fetching spark://e85709d55941:34415/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: Fetching spark://e85709d55941:34415/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp6025844392272878079.tmp\n",
            "23/05/01 16:42:36 INFO Utils: /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp6025844392272878079.tmp has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/05/01 16:42:36 INFO Executor: Adding file:/tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/05/01 16:42:36 INFO Executor: Fetching spark://e85709d55941:34415/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: Fetching spark://e85709d55941:34415/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp449541896844574360.tmp\n",
            "23/05/01 16:42:36 INFO Utils: /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp449541896844574360.tmp has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/05/01 16:42:36 INFO Executor: Adding file:/tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/05/01 16:42:36 INFO Executor: Fetching spark://e85709d55941:34415/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: Fetching spark://e85709d55941:34415/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp9173690434825917697.tmp\n",
            "23/05/01 16:42:36 INFO Utils: /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp9173690434825917697.tmp has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/05/01 16:42:36 INFO Executor: Adding file:/tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/05/01 16:42:36 INFO Executor: Fetching spark://e85709d55941:34415/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: Fetching spark://e85709d55941:34415/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp5943828419464944891.tmp\n",
            "23/05/01 16:42:36 INFO Utils: /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp5943828419464944891.tmp has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/05/01 16:42:36 INFO Executor: Adding file:/tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/05/01 16:42:36 INFO Executor: Fetching spark://e85709d55941:34415/jars/log4j_log4j-1.2.17.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: Fetching spark://e85709d55941:34415/jars/log4j_log4j-1.2.17.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp3427243655082828802.tmp\n",
            "23/05/01 16:42:36 INFO Utils: /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp3427243655082828802.tmp has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/log4j_log4j-1.2.17.jar\n",
            "23/05/01 16:42:36 INFO Executor: Adding file:/tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/log4j_log4j-1.2.17.jar to class loader\n",
            "23/05/01 16:42:36 INFO Executor: Fetching spark://e85709d55941:34415/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: Fetching spark://e85709d55941:34415/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp7537788420587874752.tmp\n",
            "23/05/01 16:42:36 INFO Utils: /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp7537788420587874752.tmp has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/05/01 16:42:36 INFO Executor: Adding file:/tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/05/01 16:42:36 INFO Executor: Fetching spark://e85709d55941:34415/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: Fetching spark://e85709d55941:34415/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp608770611802458545.tmp\n",
            "23/05/01 16:42:36 INFO Utils: /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp608770611802458545.tmp has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/05/01 16:42:36 INFO Executor: Adding file:/tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/05/01 16:42:36 INFO Executor: Fetching spark://e85709d55941:34415/jars/com.101tec_zkclient-0.3.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: Fetching spark://e85709d55941:34415/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp2151839162733616744.tmp\n",
            "23/05/01 16:42:36 INFO Utils: /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp2151839162733616744.tmp has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/com.101tec_zkclient-0.3.jar\n",
            "23/05/01 16:42:36 INFO Executor: Adding file:/tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/05/01 16:42:36 INFO Executor: Fetching spark://e85709d55941:34415/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: Fetching spark://e85709d55941:34415/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp5722208300659032037.tmp\n",
            "23/05/01 16:42:36 INFO Utils: /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp5722208300659032037.tmp has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/05/01 16:42:36 INFO Executor: Adding file:/tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/05/01 16:42:36 INFO Executor: Fetching spark://e85709d55941:34415/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: Fetching spark://e85709d55941:34415/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp7988627981798722673.tmp\n",
            "23/05/01 16:42:36 INFO Utils: /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp7988627981798722673.tmp has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/05/01 16:42:36 INFO Executor: Adding file:/tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/05/01 16:42:36 INFO Executor: Fetching spark://e85709d55941:34415/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682959352594\n",
            "23/05/01 16:42:36 INFO Utils: Fetching spark://e85709d55941:34415/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp585563692990652556.tmp\n",
            "23/05/01 16:42:36 INFO Utils: /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/fetchFileTemp585563692990652556.tmp has been previously copied to /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/05/01 16:42:36 INFO Executor: Adding file:/tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/userFiles-833e55b8-f971-444e-a27e-15d538eb909b/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/05/01 16:42:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41285.\n",
            "23/05/01 16:42:36 INFO NettyBlockTransferService: Server created on e85709d55941:41285\n",
            "23/05/01 16:42:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/05/01 16:42:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e85709d55941, 41285, None)\n",
            "23/05/01 16:42:36 INFO BlockManagerMasterEndpoint: Registering block manager e85709d55941:41285 with 366.3 MiB RAM, BlockManagerId(driver, e85709d55941, 41285, None)\n",
            "23/05/01 16:42:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e85709d55941, 41285, None)\n",
            "23/05/01 16:42:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e85709d55941, 41285, None)\n",
            "23/05/01 16:42:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/05/01 16:42:38 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/05/01 16:42:39 INFO InMemoryFileIndex: It took 61 ms to list leaf files for 1 paths.\n",
            "23/05/01 16:42:40 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/05/01 16:42:40 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/05/01 16:42:40 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/05/01 16:42:40 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 16:42:40 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 16:42:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/05/01 16:42:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 105.0 KiB, free 366.2 MiB)\n",
            "23/05/01 16:42:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 366.2 MiB)\n",
            "23/05/01 16:42:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e85709d55941:41285 (size: 37.7 KiB, free: 366.3 MiB)\n",
            "23/05/01 16:42:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 16:42:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 16:42:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/05/01 16:42:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e85709d55941, executor driver, partition 0, PROCESS_LOCAL, 4653 bytes) taskResourceAssignments Map()\n",
            "23/05/01 16:42:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/05/01 16:42:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2363 bytes result sent to driver\n",
            "23/05/01 16:42:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1171 ms on e85709d55941 (executor driver) (1/1)\n",
            "23/05/01 16:42:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/05/01 16:42:41 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.463 s\n",
            "23/05/01 16:42:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 16:42:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/05/01 16:42:41 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.596292 s\n",
            "23/05/01 16:42:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on e85709d55941:41285 in memory (size: 37.7 KiB, free: 366.3 MiB)\n",
            "23/05/01 16:42:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(XDSegID),EqualTo(XDSegID,1.524393684E9)\n",
            "23/05/01 16:42:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(XDSegID#12),(XDSegID#12 = 1.524393684E9)\n",
            "23/05/01 16:42:45 INFO FileSourceStrategy: Output Data Schema: struct<time_local: timestamp, XDSegID: double>\n",
            "23/05/01 16:42:46 INFO CodeGenerator: Code generated in 308.551711 ms\n",
            "23/05/01 16:42:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 352.8 KiB, free 366.0 MiB)\n",
            "23/05/01 16:42:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 365.9 MiB)\n",
            "23/05/01 16:42:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e85709d55941:41285 (size: 35.7 KiB, free: 366.3 MiB)\n",
            "23/05/01 16:42:46 INFO SparkContext: Created broadcast 1 from collect at /content/road_density_incidents.py:27\n",
            "23/05/01 16:42:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/05/01 16:42:46 INFO SparkContext: Starting job: collect at /content/road_density_incidents.py:27\n",
            "23/05/01 16:42:46 INFO DAGScheduler: Got job 1 (collect at /content/road_density_incidents.py:27) with 1 output partitions\n",
            "23/05/01 16:42:46 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /content/road_density_incidents.py:27)\n",
            "23/05/01 16:42:46 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/05/01 16:42:46 INFO DAGScheduler: Missing parents: List()\n",
            "23/05/01 16:42:46 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at collect at /content/road_density_incidents.py:27), which has no missing parents\n",
            "23/05/01 16:42:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.2 KiB, free 365.9 MiB)\n",
            "23/05/01 16:42:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 365.9 MiB)\n",
            "23/05/01 16:42:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on e85709d55941:41285 (size: 6.4 KiB, free: 366.3 MiB)\n",
            "23/05/01 16:42:46 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/05/01 16:42:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at collect at /content/road_density_incidents.py:27) (first 15 tasks are for partitions Vector(0))\n",
            "23/05/01 16:42:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/05/01 16:42:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (e85709d55941, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()\n",
            "23/05/01 16:42:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/05/01 16:42:46 INFO FileScanRDD: Reading File path: file:///content/incidents_10000.parquet/part-00000-d80f9833-bdf8-4987-8078-097de2f31580-c000.snappy.parquet, range: 0-709306, partition values: [empty row]\n",
            "23/05/01 16:42:46 INFO FilterCompat: Filtering using predicate: and(noteq(XDSegID, null), eq(XDSegID, 1.524393684E9))\n",
            "23/05/01 16:42:46 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/05/01 16:42:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1817 bytes result sent to driver\n",
            "23/05/01 16:42:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 893 ms on e85709d55941 (executor driver) (1/1)\n",
            "23/05/01 16:42:47 INFO DAGScheduler: ResultStage 1 (collect at /content/road_density_incidents.py:27) finished in 0.932 s\n",
            "23/05/01 16:42:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/05/01 16:42:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/05/01 16:42:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/05/01 16:42:47 INFO DAGScheduler: Job 1 finished: collect at /content/road_density_incidents.py:27, took 0.957047 s\n",
            "s3://bigdatafinal-hannah-lucas/output/incidents_hourly_density_1524393684.json\n",
            "23/05/01 16:42:47 INFO SparkUI: Stopped Spark web UI at http://e85709d55941:4040\n",
            "23/05/01 16:42:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/05/01 16:42:47 INFO MemoryStore: MemoryStore cleared\n",
            "23/05/01 16:42:47 INFO BlockManager: BlockManager stopped\n",
            "23/05/01 16:42:47 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/05/01 16:42:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/05/01 16:42:47 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/05/01 16:42:47 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/05/01 16:42:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5\n",
            "23/05/01 16:42:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-e561f716-ef76-42de-a8a2-bb5d3bb21092\n",
            "23/05/01 16:42:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-99fa4e6d-5533-49ec-87f9-c310189804f5/pyspark-0e6d1d88-dc8e-4acd-a047-cba417a50664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='road_density_incidents.py', Bucket='bigdatafinal-hannah-lucas', Key='road_density_incidents.py')"
      ],
      "metadata": {
        "id": "Eawiq24Wpx6f"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_job_2(app_name='road_density_incidents', pyfile_uri='s3://bigdatafinal-hannah-lucas/road_density_incidents.py', XDSegID = XDSEGID)"
      ],
      "metadata": {
        "id": "DWCuMmPepyNb"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. query the average response time for each road segment, and sort it to find\n",
        "#  which road segments have the highest and lowest response times. \n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from operator import add\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "sc = SparkContext(conf=conf).getOrCreate()\n",
        "spark = SparkSession(sc)\n"
      ],
      "metadata": {
        "id": "FK1csRJH4-Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incidents = spark.read.parquet(\"incidents_10000.parquet\")\n",
        "\n",
        "incidents = incidents.drop(\"latitude\", \"longitude\", \"emdCardNumber\", \"time_utc\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"geometry\")\n",
        "incidents = incidents.na.drop(subset=[\"response_time_sec\",\"XDSegID\"])"
      ],
      "metadata": {
        "id": "fDbQ3mWd7wmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "incidents = incidents.rdd\n",
        "Seg_response_sum = incidents.map(lambda x: (x.XDSegID, x.response_time_sec)).reduceByKey(lambda a, b: a + b)\n",
        "Seg_count = incidents.map(lambda x: (x.XDSegID, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "Seg_response_sum.take(10)\n",
        "Seg_count.take(10)"
      ],
      "metadata": {
        "id": "GXZ8Q6v25H3s",
        "outputId": "3887950b-873b-451b-e70f-919b7cfd541e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1524393684.0, 12),\n",
              " (1524356434.0, 3),\n",
              " (449620819.0, 1),\n",
              " (396068370.0, 2),\n",
              " (156041062.0, 2),\n",
              " (1524450692.0, 16),\n",
              " (1524630585.0, 8),\n",
              " (450433846.0, 3),\n",
              " (1524470172.0, 3),\n",
              " (449631045.0, 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "join = Seg_response_sum.join(Seg_count)\n",
        "output = join.map(lambda a: (a[0], a[1][0]/a[1][1]))\n",
        "output = output.sortBy(lambda x: x[1], ascending=False)\n",
        "output.take(25)"
      ],
      "metadata": {
        "id": "hT2q64_r9Bbl",
        "outputId": "4b12312e-c868-4f41-c078-addbe04a59ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1524369685.0, 1550.0),\n",
              " (160847340.0, 1384.0),\n",
              " (1524578575.0, 1367.0),\n",
              " (449617585.0, 1356.0),\n",
              " (1524435487.0, 1283.0),\n",
              " (1524353049.0, 1275.0),\n",
              " (1524321749.0, 1228.6666666666667),\n",
              " (1524596883.0, 1215.0),\n",
              " (449620896.0, 1179.0),\n",
              " (449633453.0, 1163.0),\n",
              " (1524321798.0, 1123.0),\n",
              " (449620756.0, 1117.0),\n",
              " (1524393513.0, 1058.0),\n",
              " (449617681.0, 968.0),\n",
              " (450428426.0, 960.0),\n",
              " (396083671.0, 949.0),\n",
              " (450430876.0, 942.0),\n",
              " (449633533.0, 936.0),\n",
              " (1524292955.0, 934.0),\n",
              " (1524308532.0, 930.75),\n",
              " (449629747.0, 930.0),\n",
              " (1524416516.0, 923.0),\n",
              " (1524393609.0, 918.0),\n",
              " (1524282578.0, 901.0),\n",
              " (1524321726.0, 899.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCpDY6nYAMaJ",
        "outputId": "458ccfe6-5541-4453-d963-ab40ef5f723b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------------+\n",
            "|       XSegID|Average Response Time|\n",
            "+-------------+---------------------+\n",
            "|1.524369685E9|               1550.0|\n",
            "|  1.6084734E8|               1384.0|\n",
            "|1.524578575E9|               1367.0|\n",
            "| 4.49617585E8|               1356.0|\n",
            "|1.524435487E9|               1283.0|\n",
            "|1.524353049E9|               1275.0|\n",
            "|1.524321749E9|   1228.6666666666667|\n",
            "|1.524596883E9|               1215.0|\n",
            "| 4.49620896E8|               1179.0|\n",
            "| 4.49633453E8|               1163.0|\n",
            "|1.524321798E9|               1123.0|\n",
            "| 4.49620756E8|               1117.0|\n",
            "|1.524393513E9|               1058.0|\n",
            "| 4.49617681E8|                968.0|\n",
            "| 4.50428426E8|                960.0|\n",
            "| 3.96083671E8|                949.0|\n",
            "| 4.50430876E8|                942.0|\n",
            "| 4.49633533E8|                936.0|\n",
            "|1.524292955E9|                934.0|\n",
            "|1.524308532E9|               930.75|\n",
            "+-------------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "4Q_bOpgV_NL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLUSTER_ID = 'j-1SAAK40XXP11C'\n",
        "\n",
        "\n",
        "def submit_job_3(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "SVLKBT4jB5eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file road_avg_response.py\n",
        "\n",
        "# 3. query the average response time for each road segment, and sort it to find\n",
        "#  which road segments have the highest and lowest response times. \n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from operator import add\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('road_avg_response.py').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  \n",
        "  try:\n",
        "\n",
        "    incidents = spark.read.parquet(\"s3://bigdatafinal-hannah-lucas/datasets/nfd_incidents_xd_seg.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"latitude\", \"longitude\", \"emdCardNumber\", \"time_utc\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"geometry\")\n",
        "    incidents = incidents.na.drop(subset=[\"response_time_sec\",\"XDSegID\"])\n",
        "    incidents = incidents.rdd\n",
        "    seg_response_sum = incidents.map(lambda x: (x.XDSegID, x.response_time_sec)).reduceByKey(lambda a, b: a + b)\n",
        "    seg_count = incidents.map(lambda x: (x.XDSegID, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    join = seg_response_sum.join(seg_count)\n",
        "    output = join.map(lambda a: (a[0], a[1][0]/a[1][1]))\n",
        "    output = output.sortBy(lambda x: x[1], ascending=False)\n",
        "    df = output.toDF(['XSegID', 'Average Response Time'])\n",
        "    df.show()\n",
        "\n",
        "    # write the DataFrame to a CSV file\n",
        "    csv_output = df.write.csv(\"s3://bigdatafinal-hannah-lucas/output/road_avg_response.csv\", header=True)\n",
        "\n",
        "    # show the DataFrame\n",
        "    df.show()\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "5zFAgEjObWog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9dd0a2-05ad-4cc5-ab7e-c2488706af70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting road_avg_response.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='road_avg_response.py', Bucket='bigdatafinal-hannah-lucas', Key='road_avg_response.py')"
      ],
      "metadata": {
        "id": "qQtg4zv6Dbfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_job_3(app_name='road_avg_response', pyfile_uri='s3://bigdatafinal-hannah-lucas/road_avg_response.py')"
      ],
      "metadata": {
        "id": "EGrPQO1sDb-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}