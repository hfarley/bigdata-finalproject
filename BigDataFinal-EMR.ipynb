{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hfarley/bigdata-finalproject/blob/main/BigDataFinal-EMR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topics of Big Data - Final Project Spring 2023 Hannah Farley Lucas Goldman"
      ],
      "metadata": {
        "id": "B-FonF1EYKr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The s3 bucket with our results is called bigdatafinal-hannah-lucas\n",
        "\n",
        "\n",
        "*   The output of each run is stored in s3://bigdatafinal-hannah-lucas/output/\n",
        "*   The .py file for each run is saved in the bucket home directory\n",
        "* The plots created in task 1 are saved as jpg files in s3://bigdatafinal-hannah-lucas/month_incident_plots/\n",
        "\n"
      ],
      "metadata": {
        "id": "HbZ1z_32aEg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This final project includes 3 tasks which seek to use data science strategies learned in CS52-66: Topics in Big Data to understand patterns and signifcant values in the data asscoiated with traffic incidients in Davidson County, Tennesse from 2017-2022. The 3 tasks are described below:\n",
        "\n",
        "\n",
        "\n",
        "*   *Task 1*: Allows users to specify a single month and year and gets all accidents that occured during that period. Then, it displays a visualization in the form of a scatter plot where each dot represents the location of an incident in that month. \n",
        "*   *Task 2*: Allows users to specify a road segment in Davidson County by its segment ID (XDSegID), and shows the number of incidents that occured during every hour for that particular road.\n",
        "* *Task 3*: This calculates the average emergency response time for each road segment in Davidson County and then sorts the data so you can see which roads have the fastest and slowest response times. "
      ],
      "metadata": {
        "id": "aKzm6xpgZohP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This file is for running the tasks with the entire dataset using EMR. To run on your own instance, change the cluster id for each job and the locations of the s3 bucket "
      ],
      "metadata": {
        "id": "5fSWL6eoZvGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To start this notebook, run the following set up commands to install the necessary libraries"
      ],
      "metadata": {
        "id": "fzjX1ErHYD1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "PWxBkqIkZA2b"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm spark-3.2.4-bin-hadoop3.2.tgz \n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "id": "X3azW0x7ZBjp",
        "outputId": "b358121e-9466-4f6a-8c1c-46a0a9c17fce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 294136\n",
            "drwxr-xr-x  1 root root      4096 Apr 28 13:35 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "705ZGS6FZFGo",
        "outputId": "44453b2c-0578-40a3-e4a8-f3ec52cf4fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIASUSGZLKF6GFOGX7D',\n",
        "    'aws_secret_access_key': 'EbMnumT5pb0i/RI9dcFeu2Qop44GQR48OlaLE+vg',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzELH//////////wEaDE2To2Qy9UnHP8pt7CLPAZfmxL8fG+k5cx0tmveiI6hOWfeoJ2dcEGwngHQgxWNjUEGSj5ZUpJjhDL3xr8U4cPMYC5ZsBUyfVKCrqsmVqafCJlKJ7/fWMDO9py0+I7/jKuOmywri0jJBKR9zqJo1Y+KqU1c9bJ3Pj5rDA10ZSabRgPsgwrBNXX+0Tdd0m78gbkqTdLdB5BkHCyGGCZhcX4S2i0NtVb0Dgq4q23nsuCmahEAqheDHv/5pjOkNKzlodr6/ilFmmMFaqyGM2IZEA3ToOYrksYxY+nIZgMUJ7iiMtr+iBjIt1UtAty41/9MCwFj07Q+qMsvgEtFC1Keb+uXc4uNeW6A5/EBR7J50xnfKiFnZ'\n",
        "}\n"
      ],
      "metadata": {
        "id": "wMJdCuDcNOGf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "metadata": {
        "id": "-W7ljr_sNOvf",
        "outputId": "1349d3df-fb29-4029-f9b8-6a4b155f2a30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.123-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.123 (from boto3)\n",
            "  Downloading botocore-1.29.123-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3)\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.123->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.123 botocore-1.29.123 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - Monthly Incidents"
      ],
      "metadata": {
        "id": "GvNg8JCIZDUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLUSTER_ID = 'j-1VO8T4QRWO0EU'\n",
        "\n",
        "\n",
        "def submit_job_1(app_name, pyfile_uri, month, year):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri,\n",
        "                     '--monthArg', str(month),\n",
        "                     '--yearArg', str(year)],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "amoQXhZFNYsU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MONTH = 6\n",
        "YEAR = 2018"
      ],
      "metadata": {
        "id": "vrHIu8IjHsWT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file month_incidents.py\n",
        "\n",
        "\n",
        "import argparse\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import year, month\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  parser = argparse.ArgumentParser(description='Process some strings.')\n",
        "  parser.add_argument('--monthArg', type=int, choices=range(1,13), help='month (1-12)', required=True)\n",
        "  parser.add_argument('--yearArg', type=int, choices=range(2017,2022), help='year (2017-2022)', required=True)\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  \n",
        "  try:\n",
        "\n",
        "    incidents = spark.read.parquet(\"s3://bigdatafinal-hannah-lucas/datasets/nfd_incidents_xd_seg.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"emdCardNumber\", \"time_utc\", \"response_time_sec\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"Dist_to_Seg\", \"XDSegID\")\n",
        "\n",
        "    incidents = incidents.withColumn(\"year\", year(incidents[\"time_local\"]))\n",
        "    incidents = incidents.withColumn(\"month\", month(incidents[\"time_local\"]))\n",
        "    \n",
        "    filtered_incidents = incidents.filter((incidents.year == args.yearArg) & (incidents.month == args.monthArg))\n",
        "    \n",
        "    filtered_incidents.show()\n",
        "\n",
        "    \n",
        "    filtered_incidents.write.parquet(\"s3://bigdatafinal-hannah-lucas/output/incidents_{month}_{year}.parquet\".format(month=args.monthArg, year=args.yearArg))\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTV-wNskZHt4",
        "outputId": "4ea22128-8ced-47cf-cc98-b0f647125c3a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting month_incidents.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='month_incidents.py', Bucket='bigdatafinal-hannah-lucas', Key='month_incidents.py')"
      ],
      "metadata": {
        "id": "6lBbStddMmAV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_job_1(app_name='month_incidents', pyfile_uri='s3://bigdatafinal-hannah-lucas/month_incidents.py', month=MONTH, year=YEAR)"
      ],
      "metadata": {
        "id": "Qh8z2eY8M0i0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas pandas pyarrow\n",
        "!pip install s3fs"
      ],
      "metadata": {
        "id": "mDiy13AbE3zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import shapely\n",
        "import pyarrow.parquet as pq\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "bucket_name = \"bigdatafinal-hannah-lucas\"\n",
        "prefix = \"output/incidents_{month}_{year}.parquet\".format(month=MONTH, year=YEAR)\n",
        "suffix = '.parquet'\n",
        "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
        "for obj in response.get('Contents', []):\n",
        "    if obj['Key'].endswith(suffix):\n",
        "      key = obj['Key']\n",
        "\n",
        "# Read the Parquet file from S3\n",
        "parquet_file = s3.get_object(Bucket=bucket_name, Key=key)\n",
        "\n",
        "table = pq.read_table(BytesIO(parquet_file['Body'].read()))\n",
        "incidents = table.to_pandas()\n",
        "\n",
        "incidents['geometry']=incidents.geometry.apply(shapely.wkt.loads)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.scatter(incidents['longitude'], incidents['latitude'])\n",
        "ax.set_title('NFD Incidents')\n",
        "ax.set_xlabel('Longitude')\n",
        "ax.set_ylabel('Latitude')\n",
        "\n",
        "\n",
        "# Save the plot as jpg file\n",
        "buf = io.BytesIO()\n",
        "plt.savefig(buf, format='jpg')\n",
        "buf.seek(0)\n",
        "\n",
        "plot_name = 'month_incident_plots/{month}_{year}.jpg'.format(month=MONTH, year=YEAR)\n",
        "\n",
        "s3.upload_fileobj(buf, Bucket=bucket_name, Key=plot_name)"
      ],
      "metadata": {
        "id": "n_l1eNGUa3BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 - Road Density Incidents"
      ],
      "metadata": {
        "id": "ZRVBgXkpZOju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLUSTER_ID = 'j-1VO8T4QRWO0EU'\n",
        "\n",
        "\n",
        "def submit_job_2(app_name, pyfile_uri, XDSegID):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri,\n",
        "                     '--XDSegID', str(XDSegID)],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "ZgMlXpKufPq_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XDSEGID = str(441552606.0)"
      ],
      "metadata": {
        "id": "FUD6QeexqKe-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file road_density_incidents.py\n",
        "\n",
        "import argparse\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import hour\n",
        "import json\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser(description='Process some strings.')\n",
        "  parser.add_argument('--XDSegID', type=float, help='Enter desired XDSegID', required=True)\n",
        "  args = parser.parse_args()\n",
        "  \n",
        "  conf = SparkConf().setAppName('road_density_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "  try:\n",
        "    incidents = spark.read.parquet(\"s3://bigdatafinal-hannah-lucas/datasets/nfd_incidents_xd_seg.parquet\")\n",
        "\n",
        "\n",
        "    incidents = incidents.drop(\"emdCardNumber\", \"time_utc\", \"latitude\", \"longitude\", \"response_time_sec\", \"geometry\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"Dist_to_Seg\")\n",
        "    incidents = incidents.withColumn(\"hour\", hour(incidents[\"time_local\"]))\n",
        "    ex_id = args.XDSegID\n",
        "    incidents = incidents.drop(\"time_local\")\n",
        "    incidents = incidents.filter(incidents.XDSegID == ex_id)\n",
        "\n",
        "    filename = \"s3://bigdatafinal-hannah-lucas/output/incidents_hourly_density_{XDSegID}.parquet\".format(XDSegID=int(float(args.XDSegID)))\n",
        "\n",
        "    incidents.write.parquet(filename)\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "nlvh4p56ZrQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab15897-ccf6-4a99-fc19-80f33402e5ad"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting road_density_incidents.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 road_density_incidents.py --XDSegID 1524393684.0"
      ],
      "metadata": {
        "id": "FM7XGvSg7IDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='road_density_incidents.py', Bucket='bigdatafinal-hannah-lucas', Key='road_density_incidents.py')"
      ],
      "metadata": {
        "id": "Eawiq24Wpx6f"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_job_2(app_name='road_density_incidents', pyfile_uri='s3://bigdatafinal-hannah-lucas/road_density_incidents.py', XDSegID = XDSEGID)"
      ],
      "metadata": {
        "id": "DWCuMmPepyNb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 - Average Road Response Time"
      ],
      "metadata": {
        "id": "qUP1HDSxZa1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLUSTER_ID = 'j-1SAAK40XXP11C'\n",
        "\n",
        "\n",
        "def submit_job_3(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "SVLKBT4jB5eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file road_avg_response.py\n",
        "\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from operator import add\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('road_avg_response.py').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  \n",
        "  try:\n",
        "\n",
        "    incidents = spark.read.parquet(\"s3://bigdatafinal-hannah-lucas/datasets/nfd_incidents_xd_seg.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"latitude\", \"longitude\", \"emdCardNumber\", \"time_utc\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"geometry\")\n",
        "    incidents = incidents.na.drop(subset=[\"response_time_sec\",\"XDSegID\"])\n",
        "    incidents = incidents.rdd\n",
        "    seg_response_sum = incidents.map(lambda x: (x.XDSegID, x.response_time_sec)).reduceByKey(lambda a, b: a + b)\n",
        "    seg_count = incidents.map(lambda x: (x.XDSegID, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    join = seg_response_sum.join(seg_count)\n",
        "    output = join.map(lambda a: (a[0], a[1][0]/a[1][1]))\n",
        "    output = output.sortBy(lambda x: x[1], ascending=False)\n",
        "    df = output.toDF(['XSegID', 'Average Response Time'])\n",
        "    df.show()\n",
        "\n",
        "    # write the DataFrame to a CSV file\n",
        "    csv_output = df.write.csv(\"s3://bigdatafinal-hannah-lucas/output/road_avg_response.csv\", header=True)\n",
        "\n",
        "    # show the DataFrame\n",
        "    df.show()\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "5zFAgEjObWog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9dd0a2-05ad-4cc5-ab7e-c2488706af70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting road_avg_response.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='road_avg_response.py', Bucket='bigdatafinal-hannah-lucas', Key='road_avg_response.py')"
      ],
      "metadata": {
        "id": "qQtg4zv6Dbfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_job_3(app_name='road_avg_response', pyfile_uri='s3://bigdatafinal-hannah-lucas/road_avg_response.py')"
      ],
      "metadata": {
        "id": "EGrPQO1sDb-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}