{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hfarley/bigdata-finalproject/blob/main/BigDataFinal-EMR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "PWxBkqIkZA2b"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm spark-3.2.4-bin-hadoop3.2.tgz \n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "id": "X3azW0x7ZBjp",
        "outputId": "41f4ca83-8972-4308-9463-2129135f46b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 294136\n",
            "drwxr-xr-x  1 root root      4096 Apr 27 13:35 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "705ZGS6FZFGo",
        "outputId": "12ca02f8-09d8-477c-9a43-d27d5dbe7a76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIASUSGZLKFSTCOJZNI',\n",
        "    'aws_secret_access_key': 'Vn3UlTeyOy2+V+tTbOQVKBVfztpeh34B8yHxKywp',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzEIf//////////wEaDNseOjscFNXZGHMpbSLPAfq5Ni8CFFx13QzW77Syt6xlNik6r9fDSye5VLevyCbROP4jgGhFey8iDFwUEGPYOUI6jRlO3EXx/Trvu1JlfiIEVVU76Zp1X0STE+68IrdHz/bhLenXsKW4N/6yp5fTASx7T3y1RDTfsRDPa673eNhIh9oZO5Y9H0Ln9qK905fIM2uyhFsEPQslu2Q1WaP6qasouxStAmWlSE8lj8VYRMefx4A9JjxOU5LidtzvWn1ppBqDqNcHMBApvz0NFGZNk0QHZCWuJ1hQjzTkiIHswyjrlraiBjItx2OhrHppzP53v525UVkIzhWKY7pJzHFtTIlcPeO7rHAdaa5ks+qZpI6f35tF'\n",
        "}"
      ],
      "metadata": {
        "id": "wMJdCuDcNOGf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLUSTER_ID = 'j-1FCUU6HNPWF75'\n",
        "\n",
        "\n",
        "def submit_job_1(app_name, pyfile_uri, month, year):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri,\n",
        "                     '--monthArg', str(month),\n",
        "                     '--yearArg', str(year)],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "amoQXhZFNYsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "metadata": {
        "id": "-W7ljr_sNOvf",
        "outputId": "f7d884b2-ffc7-48a1-ad02-d08cf3295a89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.123-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.123\n",
            "  Downloading botocore-1.29.123-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.123->boto3) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.123->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.123 botocore-1.29.123 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. query any month of any year within the database and it will show a map of the incidents during that month. \n",
        "# 2. input a road segment, and it will show a graph of the density of the times \n",
        "# that incidents occur in that road segment. \n",
        "# 3. query the average response time for each road segment, and sort it to find\n",
        "#  which road segments have the highest and lowest response times. \n",
        "\n"
      ],
      "metadata": {
        "id": "9oadSQOUZZ1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes: \n",
        "\n",
        "\n",
        "*   .show() will give the first 20 entries\n",
        "*   in the spark assignment we only used SparkContext, but here we also need Spark Session\n",
        "\n",
        "For the first task we need ID, latitude & longitude (or, alternatively geometry), and one of the time variables - I decided to use the local time variable since that would be more meaningful for someone trying to visualize the data\n",
        "\n",
        "We also add columns for month and year (based on time_local) for querying purposes\n",
        "\n",
        "* Need to decide how we want to visualize this and get the coordinates to draw a map of Davidson county which then shows the incidents\n",
        "* Month & year are specified as command line arguments\n",
        "\n"
      ],
      "metadata": {
        "id": "DWEy3kv2nSYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MONTH = 1\n",
        "YEAR = 2021"
      ],
      "metadata": {
        "id": "vrHIu8IjHsWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file month_incidents.py\n",
        "\n",
        "\n",
        "import argparse\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import year, month\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  parser = argparse.ArgumentParser(description='Process some strings.')\n",
        "  parser.add_argument('--monthArg', type=int, choices=range(1,13), help='month (1-12)', required=True)\n",
        "  parser.add_argument('--yearArg', type=int, choices=range(2017,2022), help='year (2017-2022)', required=True)\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  \n",
        "  try:\n",
        "\n",
        "    incidents = spark.read.parquet(\"s3://bigdatafinal-hannah-lucas/datasets/nfd_incidents_xd_seg.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"emdCardNumber\", \"time_utc\", \"response_time_sec\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"Dist_to_Seg\", \"XDSegID\")\n",
        "\n",
        "    incidents = incidents.withColumn(\"year\", year(incidents[\"time_local\"]))\n",
        "    incidents = incidents.withColumn(\"month\", month(incidents[\"time_local\"]))\n",
        "    \n",
        "    filtered_incidents = incidents.filter((incidents.year == args.yearArg) & (incidents.month == args.monthArg))\n",
        "    \n",
        "    filtered_incidents.show()\n",
        "\n",
        "    \n",
        "    filtered_incidents.write.parquet(\"s3://bigdatafinal-hannah-lucas/output/incidents_{month}_{year}.parquet\".format(month=args.monthArg, year=args.yearArg))\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTV-wNskZHt4",
        "outputId": "07da6965-1e2b-48cf-934e-7b1730567054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting month_incidents.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='month_incidents.py', Bucket='bigdatafinal-hannah-lucas', Key='month_incidents.py')"
      ],
      "metadata": {
        "id": "6lBbStddMmAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_job_1(app_name='month_incidents', pyfile_uri='s3://bigdatafinal-hannah-lucas/month_incidents.py', month=MONTH, year=YEAR)"
      ],
      "metadata": {
        "id": "Qh8z2eY8M0i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas pandas pyarrow\n",
        "!pip install s3fs"
      ],
      "metadata": {
        "id": "mDiy13AbE3zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import shapely\n",
        "import pyarrow.parquet as pq\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "bucket_name = \"bigdatafinal-hannah-lucas\"\n",
        "prefix = \"output/incidents_{month}_{year}.parquet\".format(month=MONTH, year=YEAR)\n",
        "suffix = '.parquet'\n",
        "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
        "for obj in response.get('Contents', []):\n",
        "    if obj['Key'].endswith(suffix):\n",
        "      key = obj['Key']\n",
        "\n",
        "# Read the Parquet file from S3\n",
        "parquet_file = s3.get_object(Bucket=bucket_name, Key=key)\n",
        "\n",
        "table = pq.read_table(BytesIO(parquet_file['Body'].read()))\n",
        "incidents = table.to_pandas()\n",
        "\n",
        "incidents['geometry']=incidents.geometry.apply(shapely.wkt.loads)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.scatter(incidents['longitude'], incidents['latitude'])\n",
        "ax.set_title('NFD Incidents')\n",
        "ax.set_xlabel('Longitude')\n",
        "ax.set_ylabel('Latitude')\n",
        "\n",
        "\n",
        "# Save the plot as jpg file\n",
        "buf = io.BytesIO()\n",
        "plt.savefig(buf, format='jpg')\n",
        "buf.seek(0)\n",
        "\n",
        "plot_name = 'month_incident_plots/{month}_{year}.jpg'.format(month=MONTH, year=YEAR)\n",
        "\n",
        "s3.upload_fileobj(buf, Bucket=bucket_name, Key=plot_name)"
      ],
      "metadata": {
        "id": "n_l1eNGUa3BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLUSTER_ID = 'j-1SAAK40XXP11C'\n",
        "\n",
        "\n",
        "def submit_job_2(app_name, pyfile_uri, XDSegID):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri,\n",
        "                     '--XDSegID', str(XDSegID)],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "ZgMlXpKufPq_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XDSEGID = 1524393684.0"
      ],
      "metadata": {
        "id": "FUD6QeexqKe-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file road_density_incidents.py\n",
        "\n",
        "import argparse\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import hour\n",
        "import plotly.express as px\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "  parser = argparse.ArgumentParser(description='Process some strings.')\n",
        "  parser.add_argument('--XDSegID', type=float, help='Enter desired XDSegID', required=True)\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('road_density_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "\n",
        "\n",
        "  try:\n",
        "    incidents = spark.read.parquet(\"nfd_incidents_xd_seg.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"emdCardNumber\", \"time_utc\", \"latitude\", \"longitude\", \"response_time_sec\", \"geometry\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"Dist_to_Seg\")\n",
        "    incidents = incidents.withColumn(\"hour\", hour(incidents[\"time_local\"]))\n",
        "    ex_id = args.XDSegID\n",
        "    incidents = incidents.drop(\"time_local\")\n",
        "    incidents = incidents.filter(incidents.XDSegID == ex_id)\n",
        "    hours = [val.hour for val in incidents.select('hour').collect()]\n",
        "\n",
        "    fig = px.histogram(hours, nbins=24, title=f\"Incident Hourly Density at Road Segment {ex_id}\")\n",
        "    fig.update_layout(\n",
        "        xaxis_title=\"Hour\", yaxis_title=\"Count\"\n",
        "    )\n",
        "    fig.write_html(\"incident_segment_hour_density.html\")\n",
        "  \n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "nlvh4p56ZrQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceca3c88-0064-40d4-e75f-6ef3e61971cb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting road_density_incidents.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 road_density_incidents.py --XDSegID 1524393684.0"
      ],
      "metadata": {
        "id": "FM7XGvSg7IDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='road_density_incidents.py', Bucket='bigdatafinal-hannah-lucas', Key='road_density_incidents.py')"
      ],
      "metadata": {
        "id": "Eawiq24Wpx6f"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_job_2(app_name='road_density_incidents', pyfile_uri='s3://bigdatafinal-hannah-lucas/road_density_incidents.py', XDSegID = XDSEGID)"
      ],
      "metadata": {
        "id": "DWCuMmPepyNb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. query the average response time for each road segment, and sort it to find\n",
        "#  which road segments have the highest and lowest response times. \n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from operator import add\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "sc = SparkContext(conf=conf).getOrCreate()\n",
        "spark = SparkSession(sc)\n"
      ],
      "metadata": {
        "id": "FK1csRJH4-Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incidents = spark.read.parquet(\"incidents_10000.parquet\")\n",
        "\n",
        "incidents = incidents.drop(\"latitude\", \"longitude\", \"emdCardNumber\", \"time_utc\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"geometry\")\n",
        "incidents = incidents.na.drop(subset=[\"response_time_sec\",\"XDSegID\"])"
      ],
      "metadata": {
        "id": "fDbQ3mWd7wmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "incidents = incidents.rdd\n",
        "Seg_response_sum = incidents.map(lambda x: (x.XDSegID, x.response_time_sec)).reduceByKey(lambda a, b: a + b)\n",
        "Seg_count = incidents.map(lambda x: (x.XDSegID, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "Seg_response_sum.take(10)\n",
        "Seg_count.take(10)"
      ],
      "metadata": {
        "id": "GXZ8Q6v25H3s",
        "outputId": "3887950b-873b-451b-e70f-919b7cfd541e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1524393684.0, 12),\n",
              " (1524356434.0, 3),\n",
              " (449620819.0, 1),\n",
              " (396068370.0, 2),\n",
              " (156041062.0, 2),\n",
              " (1524450692.0, 16),\n",
              " (1524630585.0, 8),\n",
              " (450433846.0, 3),\n",
              " (1524470172.0, 3),\n",
              " (449631045.0, 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "join = Seg_response_sum.join(Seg_count)\n",
        "output = join.map(lambda a: (a[0], a[1][0]/a[1][1]))\n",
        "output = output.sortBy(lambda x: x[1], ascending=False)\n",
        "output.take(25)"
      ],
      "metadata": {
        "id": "hT2q64_r9Bbl",
        "outputId": "4b12312e-c868-4f41-c078-addbe04a59ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1524369685.0, 1550.0),\n",
              " (160847340.0, 1384.0),\n",
              " (1524578575.0, 1367.0),\n",
              " (449617585.0, 1356.0),\n",
              " (1524435487.0, 1283.0),\n",
              " (1524353049.0, 1275.0),\n",
              " (1524321749.0, 1228.6666666666667),\n",
              " (1524596883.0, 1215.0),\n",
              " (449620896.0, 1179.0),\n",
              " (449633453.0, 1163.0),\n",
              " (1524321798.0, 1123.0),\n",
              " (449620756.0, 1117.0),\n",
              " (1524393513.0, 1058.0),\n",
              " (449617681.0, 968.0),\n",
              " (450428426.0, 960.0),\n",
              " (396083671.0, 949.0),\n",
              " (450430876.0, 942.0),\n",
              " (449633533.0, 936.0),\n",
              " (1524292955.0, 934.0),\n",
              " (1524308532.0, 930.75),\n",
              " (449629747.0, 930.0),\n",
              " (1524416516.0, 923.0),\n",
              " (1524393609.0, 918.0),\n",
              " (1524282578.0, 901.0),\n",
              " (1524321726.0, 899.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCpDY6nYAMaJ",
        "outputId": "458ccfe6-5541-4453-d963-ab40ef5f723b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------------+\n",
            "|       XSegID|Average Response Time|\n",
            "+-------------+---------------------+\n",
            "|1.524369685E9|               1550.0|\n",
            "|  1.6084734E8|               1384.0|\n",
            "|1.524578575E9|               1367.0|\n",
            "| 4.49617585E8|               1356.0|\n",
            "|1.524435487E9|               1283.0|\n",
            "|1.524353049E9|               1275.0|\n",
            "|1.524321749E9|   1228.6666666666667|\n",
            "|1.524596883E9|               1215.0|\n",
            "| 4.49620896E8|               1179.0|\n",
            "| 4.49633453E8|               1163.0|\n",
            "|1.524321798E9|               1123.0|\n",
            "| 4.49620756E8|               1117.0|\n",
            "|1.524393513E9|               1058.0|\n",
            "| 4.49617681E8|                968.0|\n",
            "| 4.50428426E8|                960.0|\n",
            "| 3.96083671E8|                949.0|\n",
            "| 4.50430876E8|                942.0|\n",
            "| 4.49633533E8|                936.0|\n",
            "|1.524292955E9|                934.0|\n",
            "|1.524308532E9|               930.75|\n",
            "+-------------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "4Q_bOpgV_NL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLUSTER_ID = 'j-1SAAK40XXP11C'\n",
        "\n",
        "\n",
        "def submit_job_3(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "metadata": {
        "id": "SVLKBT4jB5eX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file road_avg_response.py\n",
        "\n",
        "# 3. query the average response time for each road segment, and sort it to find\n",
        "#  which road segments have the highest and lowest response times. \n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from operator import add\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('road_avg_response.py').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  \n",
        "  try:\n",
        "\n",
        "    incidents = spark.read.parquet(\"s3://bigdatafinal-hannah-lucas/datasets/nfd_incidents_xd_seg.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"latitude\", \"longitude\", \"emdCardNumber\", \"time_utc\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"geometry\")\n",
        "    incidents = incidents.na.drop(subset=[\"response_time_sec\",\"XDSegID\"])\n",
        "    incidents = incidents.rdd\n",
        "    seg_response_sum = incidents.map(lambda x: (x.XDSegID, x.response_time_sec)).reduceByKey(lambda a, b: a + b)\n",
        "    seg_count = incidents.map(lambda x: (x.XDSegID, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    join = seg_response_sum.join(seg_count)\n",
        "    output = join.map(lambda a: (a[0], a[1][0]/a[1][1]))\n",
        "    output = output.sortBy(lambda x: x[1], ascending=False)\n",
        "    df = output.toDF(['XSegID', 'Average Response Time'])\n",
        "    df.show()\n",
        "\n",
        "    # write the DataFrame to a CSV file\n",
        "    csv_output = df.write.csv(\"s3://bigdatafinal-hannah-lucas/output/road_avg_response.csv\", header=True)\n",
        "\n",
        "    # show the DataFrame\n",
        "    df.show()\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "5zFAgEjObWog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9dd0a2-05ad-4cc5-ab7e-c2488706af70"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting road_avg_response.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3.upload_file(Filename='road_avg_response.py', Bucket='bigdatafinal-hannah-lucas', Key='road_avg_response.py')"
      ],
      "metadata": {
        "id": "qQtg4zv6Dbfz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_job_3(app_name='road_avg_response', pyfile_uri='s3://bigdatafinal-hannah-lucas/road_avg_response.py')"
      ],
      "metadata": {
        "id": "EGrPQO1sDb-I"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}