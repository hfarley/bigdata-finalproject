{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hfarley/bigdata-finalproject/blob/main/BigDataFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "PWxBkqIkZA2b"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "# !rm spark-3.2.4-bin-hadoop3.2.tgz \n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3azW0x7ZBjp",
        "outputId": "47555fe4-c9d1-4061-dd5c-f6331c46887e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 296072\n",
            "-rw-r--r--  1 root root   1975841 Apr 28 22:01 nfd_incidents_xd_seg.parquet\n",
            "drwxr-xr-x  1 root root      4096 Apr 27 13:35 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "705ZGS6FZFGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46fc97e8-3363-498a-e6d9-3796419cce91"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. query any month of any year within the database and it will show a map of the incidents during that month. \n",
        "# 2. input a road segment, and it will show a graph of the density of the times \n",
        "# that incidents occur in that road segment. \n",
        "# 3. query the average response time for each road segment, and sort it to find\n",
        "#  which road segments have the highest and lowest response times. \n",
        "\n"
      ],
      "metadata": {
        "id": "9oadSQOUZZ1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this creates a smaller sample dataset to work with locally\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"read_parquet\").getOrCreate()\n",
        "\n",
        "df = spark.read.parquet(\"nfd_incidents_xd_seg.parquet\")\n",
        "\n",
        "# create a smaller DataFrame with the first 1000 entries\n",
        "df_small = df.limit(10000)\n",
        "\n",
        "# write the output \n",
        "df_small.write.parquet(\"incidents_10000.parquet\")\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "fj6Y61ejlPk_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes: \n",
        "\n",
        "\n",
        "*   .show() will give the first 20 entries\n",
        "*   in the spark assignment we only used SparkContext, but here we also need Spark Session\n",
        "\n",
        "For the first task we need ID, latitude & longitude (or, alternatively geometry), and one of the time variables - I decided to use the local time variable since that would be more meaningful for someone trying to visualize the data\n",
        "\n",
        "We also add columns for month and year (based on time_local) for querying purposes\n",
        "\n",
        "* Need to decide how we want to visualize this and get the coordinates to draw a map of Davidson county which then shows the incidents\n",
        "* Month & year are specified as command line arguments\n",
        "\n"
      ],
      "metadata": {
        "id": "DWEy3kv2nSYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file month_incidents.py\n",
        "\n",
        "\n",
        "import argparse\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import year, month\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  parser = argparse.ArgumentParser(description='Process some strings.')\n",
        "  parser.add_argument('--monthArg', type=int, choices=range(1,13), help='month (1-12)', required=True)\n",
        "  parser.add_argument('--yearArg', type=int, choices=range(2017,2023), help='year (2017-2022)', required=True)\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  \n",
        "  try:\n",
        "\n",
        "    incidents = spark.read.parquet(\"incidents_100000.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"emdCardNumber\", \"time_utc\", \"response_time_sec\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"Dist_to_Seg\", \"XDSegID\")\n",
        "\n",
        "    incidents = incidents.withColumn(\"year\", year(incidents[\"time_local\"]))\n",
        "    incidents = incidents.withColumn(\"month\", month(incidents[\"time_local\"]))\n",
        "    \n",
        "    filtered_incidents = incidents.filter((incidents.year == args.yearArg) & (incidents.month == args.monthArg))\n",
        "    \n",
        "    filtered_incidents.show()\n",
        "    \n",
        "    # counts.repartition(1).saveAsTextFile(\"\")\n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTV-wNskZHt4",
        "outputId": "509e62c3-a40c-4814-8f34-b2cf0a8feb5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting month_incidents.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to test locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 month_incidents.py --monthArg 2 --yearArg 2017"
      ],
      "metadata": {
        "id": "rojSdKp3Zq32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In profs sample code they use plotly express, but: \n",
        "\n",
        "\n",
        "*   plotly express can only be used with pandas, which means we have to convert our spark df to a pandas df to use plotly express but obviously this is an issue with the size of our dataset\n",
        "*   to use regular plotly, we have to sign up/create an account to get an access key for their API calls to create the maps we need\n",
        "\n"
      ],
      "metadata": {
        "id": "FhHEKto2v28P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file road_density_incidents.py\n",
        "\n",
        "import argparse\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import hour\n",
        "import plotly.express as px\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  parser = argparse.ArgumentParser(description='Process some strings.')\n",
        "  parser.add_argument('--XDSegID', type=float, help='Enter desired XDSegID', required=True)\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  \n",
        "  try:\n",
        "    incidents = spark.read.parquet(\"incidents_100000.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"emdCardNumber\", \"time_utc\", \"latitude\", \"longitude\", \"response_time_sec\", \"geometry\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"Dist_to_Seg\")\n",
        "    incidents = incidents.withColumn(\"hour\", hour(incidents[\"time_local\"]))\n",
        "    ex_id = args.XDSegID\n",
        "    incidents = incidents.drop(\"time_local\")\n",
        "    incidents = incidents.filter(incidents.XDSegID == ex_id)\n",
        "    hours = [val.hour for val in incidents.select('hour').collect()]\n",
        "\n",
        "    fig = px.histogram(hours, nbins=24, title=f\"Incident Hourly Density at Road Segment {ex_id}\")\n",
        "    fig.update_layout(\n",
        "        xaxis_title=\"Hour\", yaxis_title=\"Count\"\n",
        "    )\n",
        "    fig.write_html(\"incident_segment_hour_density.html\")\n",
        "  \n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "nlvh4p56ZrQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcfa1e44-a364-4039-e921-9aa673942bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting road_density_incidents.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 road_density_incidents.py --XDSegID 1524393684.0"
      ],
      "metadata": {
        "id": "9G4EN0Z6bWDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99a3f8f4-790e-42ad-b520-ff7e7cfdb738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-707d8ca6-0822-432d-aa2d-cb481b027e89;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 671ms :: artifacts dl 18ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-707d8ca6-0822-432d-aa2d-cb481b027e89\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/12ms)\n",
            "23/04/27 22:18:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/27 22:18:17 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/27 22:18:17 INFO ResourceUtils: ==============================================================\n",
            "23/04/27 22:18:17 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/27 22:18:17 INFO ResourceUtils: ==============================================================\n",
            "23/04/27 22:18:17 INFO SparkContext: Submitted application: month_incidents\n",
            "23/04/27 22:18:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/27 22:18:17 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/27 22:18:17 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/27 22:18:17 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/27 22:18:17 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/27 22:18:17 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/27 22:18:17 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/27 22:18:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/27 22:18:17 INFO Utils: Successfully started service 'sparkDriver' on port 41863.\n",
            "23/04/27 22:18:17 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/27 22:18:17 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/27 22:18:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/27 22:18:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/27 22:18:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/27 22:18:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1b4265f9-2b2c-40f8-be31-ea0b144244eb\n",
            "23/04/27 22:18:18 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/27 22:18:18 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/27 22:18:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "23/04/27 22:18:18 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "23/04/27 22:18:18 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://55cb51bda717:4041\n",
            "23/04/27 22:18:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://55cb51bda717:41863/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://55cb51bda717:41863/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://55cb51bda717:41863/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://55cb51bda717:41863/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://55cb51bda717:41863/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://55cb51bda717:41863/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://55cb51bda717:41863/jars/com.101tec_zkclient-0.3.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://55cb51bda717:41863/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://55cb51bda717:41863/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://55cb51bda717:41863/jars/log4j_log4j-1.2.17.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://55cb51bda717:41863/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://55cb51bda717:41863/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/27 22:18:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/27 22:18:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/27 22:18:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/27 22:18:18 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/27 22:18:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/27 22:18:18 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/com.101tec_zkclient-0.3.jar\n",
            "23/04/27 22:18:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/27 22:18:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/27 22:18:18 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/log4j_log4j-1.2.17.jar\n",
            "23/04/27 22:18:18 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/27 22:18:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/27 22:18:18 INFO Executor: Starting executor ID driver on host 55cb51bda717\n",
            "23/04/27 22:18:18 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/log4j_log4j-1.2.17.jar\n",
            "23/04/27 22:18:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/27 22:18:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/27 22:18:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/27 22:18:18 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/27 22:18:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/27 22:18:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/27 22:18:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/27 22:18:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/27 22:18:18 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/27 22:18:18 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/com.101tec_zkclient-0.3.jar\n",
            "23/04/27 22:18:18 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/27 22:18:18 INFO Executor: Fetching spark://55cb51bda717:41863/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO TransportClientFactory: Successfully created connection to 55cb51bda717/172.28.0.12:41863 after 31 ms (0 ms spent in bootstraps)\n",
            "23/04/27 22:18:18 INFO Utils: Fetching spark://55cb51bda717:41863/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp5948683847632720749.tmp\n",
            "23/04/27 22:18:18 INFO Utils: /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp5948683847632720749.tmp has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/27 22:18:18 INFO Executor: Adding file:/tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/27 22:18:18 INFO Executor: Fetching spark://55cb51bda717:41863/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Fetching spark://55cb51bda717:41863/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp6226873504964535244.tmp\n",
            "23/04/27 22:18:18 INFO Utils: /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp6226873504964535244.tmp has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/27 22:18:18 INFO Executor: Adding file:/tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/27 22:18:18 INFO Executor: Fetching spark://55cb51bda717:41863/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Fetching spark://55cb51bda717:41863/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp3460820847217449673.tmp\n",
            "23/04/27 22:18:18 INFO Utils: /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp3460820847217449673.tmp has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/27 22:18:18 INFO Executor: Adding file:/tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/27 22:18:18 INFO Executor: Fetching spark://55cb51bda717:41863/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Fetching spark://55cb51bda717:41863/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp19215906055127082.tmp\n",
            "23/04/27 22:18:18 INFO Utils: /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp19215906055127082.tmp has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/27 22:18:18 INFO Executor: Adding file:/tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/27 22:18:18 INFO Executor: Fetching spark://55cb51bda717:41863/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Fetching spark://55cb51bda717:41863/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp4749717318194717859.tmp\n",
            "23/04/27 22:18:18 INFO Utils: /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp4749717318194717859.tmp has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/27 22:18:18 INFO Executor: Adding file:/tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/27 22:18:18 INFO Executor: Fetching spark://55cb51bda717:41863/jars/log4j_log4j-1.2.17.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Fetching spark://55cb51bda717:41863/jars/log4j_log4j-1.2.17.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp5855692985675606738.tmp\n",
            "23/04/27 22:18:18 INFO Utils: /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp5855692985675606738.tmp has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/log4j_log4j-1.2.17.jar\n",
            "23/04/27 22:18:18 INFO Executor: Adding file:/tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/27 22:18:18 INFO Executor: Fetching spark://55cb51bda717:41863/jars/com.101tec_zkclient-0.3.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Fetching spark://55cb51bda717:41863/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp6928645156652449016.tmp\n",
            "23/04/27 22:18:18 INFO Utils: /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp6928645156652449016.tmp has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/com.101tec_zkclient-0.3.jar\n",
            "23/04/27 22:18:18 INFO Executor: Adding file:/tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/27 22:18:18 INFO Executor: Fetching spark://55cb51bda717:41863/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Fetching spark://55cb51bda717:41863/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp3027942372939007004.tmp\n",
            "23/04/27 22:18:18 INFO Utils: /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp3027942372939007004.tmp has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/27 22:18:18 INFO Executor: Adding file:/tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/27 22:18:18 INFO Executor: Fetching spark://55cb51bda717:41863/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Fetching spark://55cb51bda717:41863/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp1274276110318244150.tmp\n",
            "23/04/27 22:18:18 INFO Utils: /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp1274276110318244150.tmp has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/27 22:18:18 INFO Executor: Adding file:/tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/27 22:18:18 INFO Executor: Fetching spark://55cb51bda717:41863/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Fetching spark://55cb51bda717:41863/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp1831067578466916107.tmp\n",
            "23/04/27 22:18:18 INFO Utils: /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp1831067578466916107.tmp has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/27 22:18:18 INFO Executor: Adding file:/tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/27 22:18:18 INFO Executor: Fetching spark://55cb51bda717:41863/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Fetching spark://55cb51bda717:41863/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp9202937733638123701.tmp\n",
            "23/04/27 22:18:18 INFO Utils: /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp9202937733638123701.tmp has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/27 22:18:18 INFO Executor: Adding file:/tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/27 22:18:18 INFO Executor: Fetching spark://55cb51bda717:41863/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682633897375\n",
            "23/04/27 22:18:18 INFO Utils: Fetching spark://55cb51bda717:41863/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp4065155847468243138.tmp\n",
            "23/04/27 22:18:19 INFO Utils: /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/fetchFileTemp4065155847468243138.tmp has been previously copied to /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/27 22:18:19 INFO Executor: Adding file:/tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/userFiles-a741d41e-6f7a-488b-827d-b334bac4565e/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/27 22:18:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37171.\n",
            "23/04/27 22:18:19 INFO NettyBlockTransferService: Server created on 55cb51bda717:37171\n",
            "23/04/27 22:18:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/27 22:18:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 55cb51bda717, 37171, None)\n",
            "23/04/27 22:18:19 INFO BlockManagerMasterEndpoint: Registering block manager 55cb51bda717:37171 with 366.3 MiB RAM, BlockManagerId(driver, 55cb51bda717, 37171, None)\n",
            "23/04/27 22:18:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 55cb51bda717, 37171, None)\n",
            "23/04/27 22:18:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 55cb51bda717, 37171, None)\n",
            "23/04/27 22:18:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/27 22:18:20 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/27 22:18:21 INFO InMemoryFileIndex: It took 49 ms to list leaf files for 1 paths.\n",
            "23/04/27 22:18:22 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/27 22:18:22 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/27 22:18:22 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/27 22:18:22 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/27 22:18:22 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/27 22:18:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/27 22:18:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 105.0 KiB, free 366.2 MiB)\n",
            "23/04/27 22:18:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 366.2 MiB)\n",
            "23/04/27 22:18:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 55cb51bda717:37171 (size: 37.7 KiB, free: 366.3 MiB)\n",
            "23/04/27 22:18:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/27 22:18:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/27 22:18:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/27 22:18:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (55cb51bda717, executor driver, partition 0, PROCESS_LOCAL, 4654 bytes) taskResourceAssignments Map()\n",
            "23/04/27 22:18:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/27 22:18:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2406 bytes result sent to driver\n",
            "23/04/27 22:18:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 946 ms on 55cb51bda717 (executor driver) (1/1)\n",
            "23/04/27 22:18:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/27 22:18:23 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.174 s\n",
            "23/04/27 22:18:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/27 22:18:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/27 22:18:23 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.256391 s\n",
            "23/04/27 22:18:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 55cb51bda717:37171 in memory (size: 37.7 KiB, free: 366.3 MiB)\n",
            "23/04/27 22:18:26 INFO FileSourceStrategy: Pushed Filters: IsNotNull(XDSegID),EqualTo(XDSegID,1.524393684E9)\n",
            "23/04/27 22:18:26 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(XDSegID#12),(XDSegID#12 = 1.524393684E9)\n",
            "23/04/27 22:18:26 INFO FileSourceStrategy: Output Data Schema: struct<time_local: timestamp, XDSegID: double>\n",
            "23/04/27 22:18:26 INFO CodeGenerator: Code generated in 218.670119 ms\n",
            "23/04/27 22:18:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 352.8 KiB, free 366.0 MiB)\n",
            "23/04/27 22:18:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 365.9 MiB)\n",
            "23/04/27 22:18:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 55cb51bda717:37171 (size: 35.7 KiB, free: 366.3 MiB)\n",
            "23/04/27 22:18:26 INFO SparkContext: Created broadcast 1 from collect at /content/road_density_incidents.py:29\n",
            "23/04/27 22:18:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/27 22:18:26 INFO SparkContext: Starting job: collect at /content/road_density_incidents.py:29\n",
            "23/04/27 22:18:26 INFO DAGScheduler: Got job 1 (collect at /content/road_density_incidents.py:29) with 1 output partitions\n",
            "23/04/27 22:18:26 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /content/road_density_incidents.py:29)\n",
            "23/04/27 22:18:26 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/27 22:18:26 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/27 22:18:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at collect at /content/road_density_incidents.py:29), which has no missing parents\n",
            "23/04/27 22:18:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.2 KiB, free 365.9 MiB)\n",
            "23/04/27 22:18:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 365.9 MiB)\n",
            "23/04/27 22:18:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 55cb51bda717:37171 (size: 6.4 KiB, free: 366.3 MiB)\n",
            "23/04/27 22:18:26 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/27 22:18:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at collect at /content/road_density_incidents.py:29) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/27 22:18:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/27 22:18:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (55cb51bda717, executor driver, partition 0, PROCESS_LOCAL, 4933 bytes) taskResourceAssignments Map()\n",
            "23/04/27 22:18:26 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/27 22:18:27 INFO FileScanRDD: Reading File path: file:///content/incidents_100000.parquet/part-00000-109dc158-d186-4c48-90ee-00cd36b4b419-c000.snappy.parquet, range: 0-1958039, partition values: [empty row]\n",
            "23/04/27 22:18:27 INFO FilterCompat: Filtering using predicate: and(noteq(XDSegID, null), eq(XDSegID, 1.524393684E9))\n",
            "23/04/27 22:18:27 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/27 22:18:27 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1896 bytes result sent to driver\n",
            "23/04/27 22:18:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 803 ms on 55cb51bda717 (executor driver) (1/1)\n",
            "23/04/27 22:18:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/27 22:18:27 INFO DAGScheduler: ResultStage 1 (collect at /content/road_density_incidents.py:29) finished in 0.832 s\n",
            "23/04/27 22:18:27 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/27 22:18:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/04/27 22:18:27 INFO DAGScheduler: Job 1 finished: collect at /content/road_density_incidents.py:29, took 0.847863 s\n",
            "23/04/27 22:18:28 INFO SparkUI: Stopped Spark web UI at http://55cb51bda717:4041\n",
            "23/04/27 22:18:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/27 22:18:28 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/27 22:18:28 INFO BlockManager: BlockManager stopped\n",
            "23/04/27 22:18:28 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/27 22:18:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/27 22:18:28 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/27 22:18:28 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/27 22:18:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582\n",
            "23/04/27 22:18:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-616e238d-c5ed-413d-994a-132b761f5582/pyspark-cd9d7190-7cd7-42cf-b94a-5e43c291220c\n",
            "23/04/27 22:18:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-178fed1e-dafe-489d-b660-97f4cadb0de5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. query the average response time for each road segment, and sort it to find\n",
        "#  which road segments have the highest and lowest response times. \n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from operator import add\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "sc = SparkContext(conf=conf).getOrCreate()\n",
        "spark = SparkSession(sc)\n"
      ],
      "metadata": {
        "id": "FK1csRJH4-Nl"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incidents = spark.read.parquet(\"incidents_10000.parquet\")\n",
        "\n",
        "incidents = incidents.drop(\"latitude\", \"longitude\", \"emdCardNumber\", \"time_utc\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"geometry\")\n",
        "incidents = incidents.na.drop(subset=[\"response_time_sec\",\"XDSegID\"])"
      ],
      "metadata": {
        "id": "fDbQ3mWd7wmg"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "incidents = incidents.rdd\n",
        "Seg_response_sum = incidents.map(lambda x: (x.XDSegID, x.response_time_sec)).reduceByKey(lambda a, b: a + b)\n",
        "Seg_count = incidents.map(lambda x: (x.XDSegID, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "Seg_response_sum.take(10)\n",
        "Seg_count.take(10)"
      ],
      "metadata": {
        "id": "GXZ8Q6v25H3s",
        "outputId": "3887950b-873b-451b-e70f-919b7cfd541e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1524393684.0, 12),\n",
              " (1524356434.0, 3),\n",
              " (449620819.0, 1),\n",
              " (396068370.0, 2),\n",
              " (156041062.0, 2),\n",
              " (1524450692.0, 16),\n",
              " (1524630585.0, 8),\n",
              " (450433846.0, 3),\n",
              " (1524470172.0, 3),\n",
              " (449631045.0, 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "join = Seg_response_sum.join(Seg_count)\n",
        "output = join.map(lambda a: (a[0], a[1][0]/a[1][1]))\n",
        "output = output.sortBy(lambda x: x[1], ascending=False)\n",
        "output.take(25)"
      ],
      "metadata": {
        "id": "hT2q64_r9Bbl",
        "outputId": "4b12312e-c868-4f41-c078-addbe04a59ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1524369685.0, 1550.0),\n",
              " (160847340.0, 1384.0),\n",
              " (1524578575.0, 1367.0),\n",
              " (449617585.0, 1356.0),\n",
              " (1524435487.0, 1283.0),\n",
              " (1524353049.0, 1275.0),\n",
              " (1524321749.0, 1228.6666666666667),\n",
              " (1524596883.0, 1215.0),\n",
              " (449620896.0, 1179.0),\n",
              " (449633453.0, 1163.0),\n",
              " (1524321798.0, 1123.0),\n",
              " (449620756.0, 1117.0),\n",
              " (1524393513.0, 1058.0),\n",
              " (449617681.0, 968.0),\n",
              " (450428426.0, 960.0),\n",
              " (396083671.0, 949.0),\n",
              " (450430876.0, 942.0),\n",
              " (449633533.0, 936.0),\n",
              " (1524292955.0, 934.0),\n",
              " (1524308532.0, 930.75),\n",
              " (449629747.0, 930.0),\n",
              " (1524416516.0, 923.0),\n",
              " (1524393609.0, 918.0),\n",
              " (1524282578.0, 901.0),\n",
              " (1524321726.0, 899.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCpDY6nYAMaJ",
        "outputId": "458ccfe6-5541-4453-d963-ab40ef5f723b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------------+\n",
            "|       XSegID|Average Response Time|\n",
            "+-------------+---------------------+\n",
            "|1.524369685E9|               1550.0|\n",
            "|  1.6084734E8|               1384.0|\n",
            "|1.524578575E9|               1367.0|\n",
            "| 4.49617585E8|               1356.0|\n",
            "|1.524435487E9|               1283.0|\n",
            "|1.524353049E9|               1275.0|\n",
            "|1.524321749E9|   1228.6666666666667|\n",
            "|1.524596883E9|               1215.0|\n",
            "| 4.49620896E8|               1179.0|\n",
            "| 4.49633453E8|               1163.0|\n",
            "|1.524321798E9|               1123.0|\n",
            "| 4.49620756E8|               1117.0|\n",
            "|1.524393513E9|               1058.0|\n",
            "| 4.49617681E8|                968.0|\n",
            "| 4.50428426E8|                960.0|\n",
            "| 3.96083671E8|                949.0|\n",
            "| 4.50430876E8|                942.0|\n",
            "| 4.49633533E8|                936.0|\n",
            "|1.524292955E9|                934.0|\n",
            "|1.524308532E9|               930.75|\n",
            "+-------------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "4Q_bOpgV_NL3"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file road_avg_response.py\n",
        "\n",
        "# 3. query the average response time for each road segment, and sort it to find\n",
        "#  which road segments have the highest and lowest response times. \n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from operator import add\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  \n",
        "  try:\n",
        "\n",
        "    incidents = spark.read.parquet(\"incidents_10000.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"latitude\", \"longitude\", \"emdCardNumber\", \"time_utc\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"geometry\")\n",
        "    incidents = incidents.na.drop(subset=[\"response_time_sec\",\"XDSegID\"])\n",
        "    incidents = incidents.rdd\n",
        "    seg_response_sum = incidents.map(lambda x: (x.XDSegID, x.response_time_sec)).reduceByKey(lambda a, b: a + b)\n",
        "    seg_count = incidents.map(lambda x: (x.XDSegID, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    join = seg_response_sum.join(seg_count)\n",
        "    output = join.map(lambda a: (a[0], a[1][0]/a[1][1]))\n",
        "    output = output.sortBy(lambda x: x[1], ascending=False)\n",
        "    df = output.toDF(['XSegID', 'Average Response Time'])\n",
        "    df.show()\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "5zFAgEjObWog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5fa10e6-d78d-49de-f979-a9020e2fdeeb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting road_avg_response.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 road_avg_response.py"
      ],
      "metadata": {
        "id": "PIOJwz7epRNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d3e71d-b5bd-48a4-e789-7dde7360041e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d6b7a40a-e73e-41e7-988a-fd14375f5886;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 806ms :: artifacts dl 27ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-d6b7a40a-e73e-41e7-988a-fd14375f5886\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/13ms)\n",
            "23/04/28 22:36:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/28 22:36:09 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/28 22:36:09 INFO ResourceUtils: ==============================================================\n",
            "23/04/28 22:36:09 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/28 22:36:09 INFO ResourceUtils: ==============================================================\n",
            "23/04/28 22:36:09 INFO SparkContext: Submitted application: month_incidents\n",
            "23/04/28 22:36:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/28 22:36:09 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/28 22:36:09 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/28 22:36:10 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/28 22:36:10 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/28 22:36:10 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/28 22:36:10 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/28 22:36:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/28 22:36:10 INFO Utils: Successfully started service 'sparkDriver' on port 45617.\n",
            "23/04/28 22:36:10 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/28 22:36:10 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/28 22:36:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/28 22:36:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/28 22:36:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/28 22:36:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fed85526-f4e7-4ae5-82b9-d9db26728b62\n",
            "23/04/28 22:36:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/28 22:36:11 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/28 22:36:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/28 22:36:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://38d47b7b9eec:4040\n",
            "23/04/28 22:36:12 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://38d47b7b9eec:45617/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://38d47b7b9eec:45617/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://38d47b7b9eec:45617/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://38d47b7b9eec:45617/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://38d47b7b9eec:45617/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://38d47b7b9eec:45617/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://38d47b7b9eec:45617/jars/com.101tec_zkclient-0.3.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://38d47b7b9eec:45617/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://38d47b7b9eec:45617/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://38d47b7b9eec:45617/jars/log4j_log4j-1.2.17.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://38d47b7b9eec:45617/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://38d47b7b9eec:45617/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/28 22:36:12 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/28 22:36:12 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/28 22:36:12 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/28 22:36:12 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/28 22:36:12 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/28 22:36:12 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/com.101tec_zkclient-0.3.jar\n",
            "23/04/28 22:36:12 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/28 22:36:12 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/28 22:36:12 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/log4j_log4j-1.2.17.jar\n",
            "23/04/28 22:36:12 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/28 22:36:12 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:12 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/28 22:36:13 INFO Executor: Starting executor ID driver on host 38d47b7b9eec\n",
            "23/04/28 22:36:13 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/log4j_log4j-1.2.17.jar\n",
            "23/04/28 22:36:13 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/28 22:36:13 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/28 22:36:13 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/28 22:36:13 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/28 22:36:13 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/28 22:36:13 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/28 22:36:13 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/28 22:36:13 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/28 22:36:13 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/28 22:36:13 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/com.101tec_zkclient-0.3.jar\n",
            "23/04/28 22:36:13 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/28 22:36:13 INFO Executor: Fetching spark://38d47b7b9eec:45617/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:13 INFO TransportClientFactory: Successfully created connection to 38d47b7b9eec/172.28.0.12:45617 after 119 ms (0 ms spent in bootstraps)\n",
            "23/04/28 22:36:13 INFO Utils: Fetching spark://38d47b7b9eec:45617/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp1643110736832014947.tmp\n",
            "23/04/28 22:36:13 INFO Utils: /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp1643110736832014947.tmp has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/28 22:36:14 INFO Executor: Adding file:/tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/28 22:36:14 INFO Executor: Fetching spark://38d47b7b9eec:45617/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:14 INFO Utils: Fetching spark://38d47b7b9eec:45617/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp8367096325051227997.tmp\n",
            "23/04/28 22:36:14 INFO Utils: /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp8367096325051227997.tmp has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/28 22:36:14 INFO Executor: Adding file:/tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/28 22:36:14 INFO Executor: Fetching spark://38d47b7b9eec:45617/jars/log4j_log4j-1.2.17.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:14 INFO Utils: Fetching spark://38d47b7b9eec:45617/jars/log4j_log4j-1.2.17.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp6899409494295581504.tmp\n",
            "23/04/28 22:36:14 INFO Utils: /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp6899409494295581504.tmp has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/log4j_log4j-1.2.17.jar\n",
            "23/04/28 22:36:14 INFO Executor: Adding file:/tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/28 22:36:14 INFO Executor: Fetching spark://38d47b7b9eec:45617/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:14 INFO Utils: Fetching spark://38d47b7b9eec:45617/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp3656642087373601986.tmp\n",
            "23/04/28 22:36:14 INFO Utils: /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp3656642087373601986.tmp has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/28 22:36:14 INFO Executor: Adding file:/tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/28 22:36:14 INFO Executor: Fetching spark://38d47b7b9eec:45617/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:14 INFO Utils: Fetching spark://38d47b7b9eec:45617/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp3695916507880457074.tmp\n",
            "23/04/28 22:36:14 INFO Utils: /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp3695916507880457074.tmp has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/28 22:36:14 INFO Executor: Adding file:/tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/28 22:36:14 INFO Executor: Fetching spark://38d47b7b9eec:45617/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:14 INFO Utils: Fetching spark://38d47b7b9eec:45617/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp3944548429434587731.tmp\n",
            "23/04/28 22:36:14 INFO Utils: /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp3944548429434587731.tmp has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/28 22:36:14 INFO Executor: Adding file:/tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/28 22:36:14 INFO Executor: Fetching spark://38d47b7b9eec:45617/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:14 INFO Utils: Fetching spark://38d47b7b9eec:45617/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp8127708101407574086.tmp\n",
            "23/04/28 22:36:14 INFO Utils: /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp8127708101407574086.tmp has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/28 22:36:14 INFO Executor: Adding file:/tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/28 22:36:14 INFO Executor: Fetching spark://38d47b7b9eec:45617/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:14 INFO Utils: Fetching spark://38d47b7b9eec:45617/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp8173836372011178915.tmp\n",
            "23/04/28 22:36:14 INFO Utils: /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp8173836372011178915.tmp has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/28 22:36:14 INFO Executor: Adding file:/tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/28 22:36:14 INFO Executor: Fetching spark://38d47b7b9eec:45617/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:14 INFO Utils: Fetching spark://38d47b7b9eec:45617/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp8359266641386003848.tmp\n",
            "23/04/28 22:36:14 INFO Utils: /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp8359266641386003848.tmp has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/28 22:36:14 INFO Executor: Adding file:/tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/28 22:36:14 INFO Executor: Fetching spark://38d47b7b9eec:45617/jars/com.101tec_zkclient-0.3.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:14 INFO Utils: Fetching spark://38d47b7b9eec:45617/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp4913554381680970149.tmp\n",
            "23/04/28 22:36:14 INFO Utils: /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp4913554381680970149.tmp has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/com.101tec_zkclient-0.3.jar\n",
            "23/04/28 22:36:14 INFO Executor: Adding file:/tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/28 22:36:14 INFO Executor: Fetching spark://38d47b7b9eec:45617/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:14 INFO Utils: Fetching spark://38d47b7b9eec:45617/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp6144705016158433436.tmp\n",
            "23/04/28 22:36:14 INFO Utils: /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp6144705016158433436.tmp has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/28 22:36:14 INFO Executor: Adding file:/tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/28 22:36:14 INFO Executor: Fetching spark://38d47b7b9eec:45617/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682721369790\n",
            "23/04/28 22:36:14 INFO Utils: Fetching spark://38d47b7b9eec:45617/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp6154158506683687437.tmp\n",
            "23/04/28 22:36:14 INFO Utils: /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/fetchFileTemp6154158506683687437.tmp has been previously copied to /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/28 22:36:14 INFO Executor: Adding file:/tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/userFiles-3424aca2-1c83-4724-8b0d-17d48a3487ba/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/28 22:36:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39643.\n",
            "23/04/28 22:36:14 INFO NettyBlockTransferService: Server created on 38d47b7b9eec:39643\n",
            "23/04/28 22:36:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/28 22:36:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 38d47b7b9eec, 39643, None)\n",
            "23/04/28 22:36:14 INFO BlockManagerMasterEndpoint: Registering block manager 38d47b7b9eec:39643 with 366.3 MiB RAM, BlockManagerId(driver, 38d47b7b9eec, 39643, None)\n",
            "23/04/28 22:36:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 38d47b7b9eec, 39643, None)\n",
            "23/04/28 22:36:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 38d47b7b9eec, 39643, None)\n",
            "23/04/28 22:36:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/28 22:36:15 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/28 22:36:16 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.\n",
            "23/04/28 22:36:17 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/28 22:36:17 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/28 22:36:17 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/28 22:36:17 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/28 22:36:17 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/28 22:36:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/28 22:36:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 105.0 KiB, free 366.2 MiB)\n",
            "23/04/28 22:36:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/28 22:36:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 38d47b7b9eec:39643 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/28 22:36:17 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/28 22:36:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/28 22:36:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/28 22:36:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (38d47b7b9eec, executor driver, partition 0, PROCESS_LOCAL, 4653 bytes) taskResourceAssignments Map()\n",
            "23/04/28 22:36:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/28 22:36:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2406 bytes result sent to driver\n",
            "23/04/28 22:36:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1225 ms on 38d47b7b9eec (executor driver) (1/1)\n",
            "23/04/28 22:36:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/28 22:36:19 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.611 s\n",
            "23/04/28 22:36:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/28 22:36:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/28 22:36:19 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.739398 s\n",
            "23/04/28 22:36:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 38d47b7b9eec:39643 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/28 22:36:22 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/28 22:36:22 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(2, response_time_sec#6, XDSegID#12)\n",
            "23/04/28 22:36:22 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, time_local: timestamp, response_time_sec: double, Dist_to_Seg: double, XDSegID: double ... 3 more fields>\n",
            "23/04/28 22:36:23 INFO CodeGenerator: Code generated in 283.452643 ms\n",
            "23/04/28 22:36:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 353.6 KiB, free 366.0 MiB)\n",
            "23/04/28 22:36:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 365.9 MiB)\n",
            "23/04/28 22:36:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 38d47b7b9eec:39643 (size: 35.9 KiB, free: 366.3 MiB)\n",
            "23/04/28 22:36:23 INFO SparkContext: Created broadcast 1 from javaToPython at NativeMethodAccessorImpl.java:0\n",
            "23/04/28 22:36:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/28 22:36:23 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166\n",
            "23/04/28 22:36:23 INFO DAGScheduler: Registering RDD 9 (reduceByKey at /content/road_avg_response.py:25) as input to shuffle 1\n",
            "23/04/28 22:36:23 INFO DAGScheduler: Registering RDD 13 (reduceByKey at /content/road_avg_response.py:26) as input to shuffle 0\n",
            "23/04/28 22:36:23 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:166) with 1 output partitions\n",
            "23/04/28 22:36:23 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at PythonRDD.scala:166)\n",
            "23/04/28 22:36:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)\n",
            "23/04/28 22:36:23 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1, ShuffleMapStage 2)\n",
            "23/04/28 22:36:23 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[9] at reduceByKey at /content/road_avg_response.py:25), which has no missing parents\n",
            "23/04/28 22:36:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 29.2 KiB, free 365.9 MiB)\n",
            "23/04/28 22:36:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 365.9 MiB)\n",
            "23/04/28 22:36:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 38d47b7b9eec:39643 (size: 12.4 KiB, free: 366.3 MiB)\n",
            "23/04/28 22:36:23 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/28 22:36:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (PairwiseRDD[9] at reduceByKey at /content/road_avg_response.py:25) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/28 22:36:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/28 22:36:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (38d47b7b9eec, executor driver, partition 0, PROCESS_LOCAL, 4921 bytes) taskResourceAssignments Map()\n",
            "23/04/28 22:36:23 INFO DAGScheduler: Submitting ShuffleMapStage 2 (PairwiseRDD[13] at reduceByKey at /content/road_avg_response.py:26), which has no missing parents\n",
            "23/04/28 22:36:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/28 22:36:23 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 29.2 KiB, free 365.9 MiB)\n",
            "23/04/28 22:36:23 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 365.8 MiB)\n",
            "23/04/28 22:36:23 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 38d47b7b9eec:39643 (size: 12.4 KiB, free: 366.2 MiB)\n",
            "23/04/28 22:36:23 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/28 22:36:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (PairwiseRDD[13] at reduceByKey at /content/road_avg_response.py:26) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/28 22:36:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "23/04/28 22:36:23 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (38d47b7b9eec, executor driver, partition 0, PROCESS_LOCAL, 4921 bytes) taskResourceAssignments Map()\n",
            "23/04/28 22:36:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "23/04/28 22:36:24 INFO FileScanRDD: Reading File path: file:///content/incidents_10000.parquet/part-00000-9eea2d40-eb7c-4361-bb7f-83ca18bae1af-c000.snappy.parquet, range: 0-709306, partition values: [empty row]\n",
            "23/04/28 22:36:24 INFO FileScanRDD: Reading File path: file:///content/incidents_10000.parquet/part-00000-9eea2d40-eb7c-4361-bb7f-83ca18bae1af-c000.snappy.parquet, range: 0-709306, partition values: [empty row]\n",
            "23/04/28 22:36:25 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/28 22:36:25 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/28 22:36:27 INFO PythonRunner: Times: total = 2945, boot = 684, init = 1692, finish = 569\n",
            "23/04/28 22:36:27 INFO PythonRunner: Times: total = 2930, boot = 693, init = 1689, finish = 548\n",
            "23/04/28 22:36:27 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2240 bytes result sent to driver\n",
            "23/04/28 22:36:27 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2197 bytes result sent to driver\n",
            "23/04/28 22:36:27 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 3859 ms on 38d47b7b9eec (executor driver) (1/1)\n",
            "23/04/28 22:36:27 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 47739\n",
            "23/04/28 22:36:27 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/28 22:36:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3907 ms on 38d47b7b9eec (executor driver) (1/1)\n",
            "23/04/28 22:36:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/28 22:36:27 INFO DAGScheduler: ShuffleMapStage 2 (reduceByKey at /content/road_avg_response.py:26) finished in 3.909 s\n",
            "23/04/28 22:36:27 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/28 22:36:27 INFO DAGScheduler: running: Set(ShuffleMapStage 1)\n",
            "23/04/28 22:36:27 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
            "23/04/28 22:36:27 INFO DAGScheduler: failed: Set()\n",
            "23/04/28 22:36:27 INFO DAGScheduler: ShuffleMapStage 1 (reduceByKey at /content/road_avg_response.py:25) finished in 4.048 s\n",
            "23/04/28 22:36:27 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/28 22:36:27 INFO DAGScheduler: running: Set()\n",
            "23/04/28 22:36:27 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
            "23/04/28 22:36:27 INFO DAGScheduler: failed: Set()\n",
            "23/04/28 22:36:27 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[19] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "23/04/28 22:36:27 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 23.0 KiB, free 365.8 MiB)\n",
            "23/04/28 22:36:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 365.8 MiB)\n",
            "23/04/28 22:36:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 38d47b7b9eec:39643 (size: 9.1 KiB, free: 366.2 MiB)\n",
            "23/04/28 22:36:27 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/28 22:36:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[19] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/28 22:36:27 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/28 22:36:27 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (38d47b7b9eec, executor driver, partition 0, NODE_LOCAL, 4461 bytes) taskResourceAssignments Map()\n",
            "23/04/28 22:36:27 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "23/04/28 22:36:28 INFO ShuffleBlockFetcherIterator: Getting 1 (26.2 KiB) non-empty blocks including 1 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/28 22:36:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 59 ms\n",
            "23/04/28 22:36:28 INFO PythonRunner: Times: total = 193, boot = -1094, init = 1280, finish = 7\n",
            "23/04/28 22:36:28 INFO ShuffleBlockFetcherIterator: Getting 1 (19.7 KiB) non-empty blocks including 1 (19.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/28 22:36:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/28 22:36:28 INFO PythonRunner: Times: total = 134, boot = 13, init = 115, finish = 6\n",
            "23/04/28 22:36:28 INFO PythonRunner: Times: total = 636, boot = -803, init = 1402, finish = 37\n",
            "23/04/28 22:36:28 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1630 bytes result sent to driver\n",
            "23/04/28 22:36:28 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 725 ms on 38d47b7b9eec (executor driver) (1/1)\n",
            "23/04/28 22:36:28 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/28 22:36:28 INFO DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:166) finished in 0.809 s\n",
            "23/04/28 22:36:28 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/28 22:36:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/28 22:36:28 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:166, took 4.915526 s\n",
            "23/04/28 22:36:29 INFO CodeGenerator: Code generated in 23.913781 ms\n",
            "23/04/28 22:36:29 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/28 22:36:29 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/28 22:36:29 INFO DAGScheduler: Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/28 22:36:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5, ShuffleMapStage 4)\n",
            "23/04/28 22:36:29 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/28 22:36:29 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[25] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/28 22:36:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 30.8 KiB, free 365.8 MiB)\n",
            "23/04/28 22:36:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 365.8 MiB)\n",
            "23/04/28 22:36:29 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 38d47b7b9eec:39643 (size: 12.4 KiB, free: 366.2 MiB)\n",
            "23/04/28 22:36:29 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/28 22:36:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/28 22:36:29 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/04/28 22:36:29 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (38d47b7b9eec, executor driver, partition 0, NODE_LOCAL, 4461 bytes) taskResourceAssignments Map()\n",
            "23/04/28 22:36:29 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)\n",
            "23/04/28 22:36:29 INFO ShuffleBlockFetcherIterator: Getting 1 (26.2 KiB) non-empty blocks including 1 (26.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/28 22:36:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms\n",
            "23/04/28 22:36:29 INFO PythonRunner: Times: total = 130, boot = -487, init = 613, finish = 4\n",
            "23/04/28 22:36:29 INFO ShuffleBlockFetcherIterator: Getting 1 (19.7 KiB) non-empty blocks including 1 (19.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/28 22:36:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/28 22:36:29 INFO PythonRunner: Times: total = 78, boot = 4, init = 70, finish = 4\n",
            "23/04/28 22:36:29 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 2543 bytes result sent to driver\n",
            "23/04/28 22:36:29 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 326 ms on 38d47b7b9eec (executor driver) (1/1)\n",
            "23/04/28 22:36:29 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/04/28 22:36:29 INFO DAGScheduler: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 0.350 s\n",
            "23/04/28 22:36:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/28 22:36:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/04/28 22:36:29 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.375579 s\n",
            "23/04/28 22:36:29 INFO CodeGenerator: Code generated in 25.917696 ms\n",
            "+-------------+---------------------+\n",
            "|       XSegID|Average Response Time|\n",
            "+-------------+---------------------+\n",
            "|1.524369685E9|               1550.0|\n",
            "|  1.6084734E8|               1384.0|\n",
            "|1.524578575E9|               1367.0|\n",
            "| 4.49617585E8|               1356.0|\n",
            "|1.524435487E9|               1283.0|\n",
            "|1.524353049E9|               1275.0|\n",
            "|1.524321749E9|   1228.6666666666667|\n",
            "|1.524596883E9|               1215.0|\n",
            "| 4.49620896E8|               1179.0|\n",
            "| 4.49633453E8|               1163.0|\n",
            "|1.524321798E9|               1123.0|\n",
            "| 4.49620756E8|               1117.0|\n",
            "|1.524393513E9|               1058.0|\n",
            "| 4.49617681E8|                968.0|\n",
            "| 4.50428426E8|                960.0|\n",
            "| 3.96083671E8|                949.0|\n",
            "| 4.50430876E8|                942.0|\n",
            "| 4.49633533E8|                936.0|\n",
            "|1.524292955E9|                934.0|\n",
            "|1.524308532E9|               930.75|\n",
            "+-------------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "23/04/28 22:36:29 INFO SparkUI: Stopped Spark web UI at http://38d47b7b9eec:4040\n",
            "23/04/28 22:36:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/28 22:36:29 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/28 22:36:29 INFO BlockManager: BlockManager stopped\n",
            "23/04/28 22:36:29 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/28 22:36:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/28 22:36:29 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/28 22:36:30 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/28 22:36:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129\n",
            "23/04/28 22:36:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-9cb836e7-a90e-4b59-94aa-1e70652afb1a\n",
            "23/04/28 22:36:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-77e02c39-bdde-45aa-80eb-90eb84b15129/pyspark-09eb0c4e-f3c8-46c0-9a8b-fb520d52aba8\n"
          ]
        }
      ]
    }
  ]
}