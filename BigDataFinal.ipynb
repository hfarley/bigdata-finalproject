{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKvOMLDfmp8OnEHq8tRohs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hfarley/bigdata-finalproject/blob/main/BigDataFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "PWxBkqIkZA2b"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "# !rm spark-3.2.4-bin-hadoop3.2.tgz \n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3azW0x7ZBjp",
        "outputId": "536d0f3c-24ec-40b3-f2f6-1dbc58f906bf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 296068\n",
            "-rw-r--r--  1 root root   1975841 Apr 27 00:09 nfd_incidents_xd_seg.parquet\n",
            "drwxr-xr-x  1 root root      4096 Apr 25 13:34 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "705ZGS6FZFGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cda754f-1dc9-44aa-cb49-1d1f830e3b30"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. query any month of any year within the database and it will show a map of the incidents during that month. \n",
        "# 2. input a road segment, and it will show a graph of the density of the times \n",
        "# that incidents occur in that road segment. \n",
        "# 3. query the average response time for each road segment, and sort it to find\n",
        "#  which road segments have the highest and lowest response times. \n",
        "\n"
      ],
      "metadata": {
        "id": "9oadSQOUZZ1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this creates a smaller sample dataset to work with locally\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"read_parquet\").getOrCreate()\n",
        "\n",
        "df = spark.read.parquet(\"nfd_incidents_xd_seg.parquet\")\n",
        "\n",
        "# create a smaller DataFrame with the first 1000 entries\n",
        "df_small = df.limit(10000)\n",
        "\n",
        "# write the output \n",
        "df_small.write.parquet(\"incidents_10000.parquet\")"
      ],
      "metadata": {
        "id": "fj6Y61ejlPk_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes: \n",
        "\n",
        "\n",
        "*   .show() will give the first 20 entries\n",
        "*   in the spark assignment we only used SparkContext, but here we also need Spark Session\n",
        "\n",
        "For the first task we need ID, latitude & longitude (or, alternatively geometry), and one of the time variables - I decided to use the local time variable since that would be more meaningful for someone trying to visualize the data\n",
        "\n",
        "We also add columns for month and year (based on time_local) for querying purposes\n",
        "\n",
        "* Need to decide how we want to visualize this and get the coordinates to draw a map of Davidson county which then shows the incidents\n",
        "* Month & year are specified as command line arguments\n",
        "\n"
      ],
      "metadata": {
        "id": "DWEy3kv2nSYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file month_incidents.py\n",
        "\n",
        "\n",
        "import argparse\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import year, month\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  parser = argparse.ArgumentParser(description='Process some strings.')\n",
        "  parser.add_argument('--monthArg', type=int, choices=range(1,13), help='month (1-12)', required=True)\n",
        "  parser.add_argument('--yearArg', type=int, choices=range(2017,2023), help='year (2017-2022)', required=True)\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  \n",
        "  try:\n",
        "\n",
        "    incidents = spark.read.parquet(\"incidents_10000.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"emdCardNumber\", \"time_utc\", \"response_time_sec\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"Dist_to_Seg\", \"XDSegID\")\n",
        "\n",
        "    incidents = incidents.withColumn(\"year\", year(incidents[\"time_local\"]))\n",
        "    incidents = incidents.withColumn(\"month\", month(incidents[\"time_local\"]))\n",
        "    \n",
        "    filtered_incidents = incidents.filter((incidents.year == args.yearArg) & (incidents.month == args.monthArg))\n",
        "    \n",
        "    filtered_incidents.show()\n",
        "    \n",
        "    # counts.repartition(1).saveAsTextFile(\"\")\n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTV-wNskZHt4",
        "outputId": "509e62c3-a40c-4814-8f34-b2cf0a8feb5f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting month_incidents.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to test locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 month_incidents.py --monthArg 2 --yearArg 2017"
      ],
      "metadata": {
        "id": "rojSdKp3Zq32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In profs sample code they use plotly express, but: \n",
        "\n",
        "\n",
        "*   plotly express can only be used with pandas, which means we have to convert our spark df to a pandas df to use plotly express but obviously this is an issue with the size of our dataset\n",
        "*   to use regular plotly, we have to sign up/create an account to get an access key for their API calls to create the maps we need\n",
        "\n"
      ],
      "metadata": {
        "id": "FhHEKto2v28P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file road_density_incidents.py\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from operator import add"
      ],
      "metadata": {
        "id": "nlvh4p56ZrQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 road_density_incidents.py"
      ],
      "metadata": {
        "id": "9G4EN0Z6bWDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file road_avg_response.py\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from operator import add"
      ],
      "metadata": {
        "id": "5zFAgEjObWog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 road_avg_response.py"
      ],
      "metadata": {
        "id": "PIOJwz7epRNV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}