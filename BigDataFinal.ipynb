{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs22Ptq068fTIM0C3D1QYs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hfarley/bigdata-finalproject/blob/main/BigDataFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "PWxBkqIkZA2b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "# !rm spark-3.2.4-bin-hadoop3.2.tgz \n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3azW0x7ZBjp",
        "outputId": "5fd0d343-8958-44bd-f96d-c41dc4748a3c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 590204\n",
            "drwxr-xr-x  2 root root      4096 Apr 27 00:11 incidents_10000.parquet\n",
            "-rw-r--r--  1 root root      1584 Apr 27 00:19 month_incidents.py\n",
            "-rw-r--r--  1 root root   1975841 Apr 27 00:09 nfd_incidents_xd_seg.parquet\n",
            "drwxr-xr-x  1 root root      4096 Apr 25 13:34 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "705ZGS6FZFGo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. query any month of any year within the database and it will show a map of the incidents during that month. \n",
        "# 2. input a road segment, and it will show a graph of the density of the times \n",
        "# that incidents occur in that road segment. \n",
        "# 3. query the average response time for each road segment, and sort it to find\n",
        "#  which road segments have the highest and lowest response times. \n",
        "\n"
      ],
      "metadata": {
        "id": "9oadSQOUZZ1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this creates a smaller sample dataset to work with locally\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"read_parquet\").getOrCreate()\n",
        "\n",
        "df = spark.read.parquet(\"nfd_incidents_xd_seg.parquet\")\n",
        "\n",
        "# create a smaller DataFrame with the first 1000 entries\n",
        "df_small = df.limit(10000)\n",
        "\n",
        "# write the output \n",
        "df_small.write.parquet(\"incidents_10000.parquet\")"
      ],
      "metadata": {
        "id": "fj6Y61ejlPk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes: \n",
        "\n",
        "\n",
        "*   .show() will give the first 20 entries\n",
        "*   in the spark assignment we only used SparkContext, but here we also need Spark Session\n",
        "\n",
        "For the first task we need ID, latitude & longitude (or, alternatively geometry), and one of the time variables - I decided to use the local time variable since that would be more meaningful for someone trying to visualize the data\n",
        "\n",
        "We also add columns for month and year (based on time_local) for querying purposes\n",
        "\n",
        "* Need to decide how we want to visualize this and get the coordinates to draw a map of Davidson county which then shows the incidents\n",
        "* Month & year are specified as command line arguments\n",
        "\n"
      ],
      "metadata": {
        "id": "DWEy3kv2nSYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file month_incidents.py\n",
        "\n",
        "\n",
        "import argparse\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import year, month\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  parser = argparse.ArgumentParser(description='Process some strings.')\n",
        "  parser.add_argument('--monthArg', type=int, choices=range(1,13), help='month (1-12)', required=True)\n",
        "  parser.add_argument('--yearArg', type=int, choices=range(2017,2023), help='year (2017-2022)', required=True)\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  \n",
        "  try:\n",
        "\n",
        "    incidents = spark.read.parquet(\"incidents_10000.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"emdCardNumber\", \"time_utc\", \"response_time_sec\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"Dist_to_Seg\", \"XDSegID\")\n",
        "\n",
        "    incidents = incidents.withColumn(\"year\", year(incidents[\"time_local\"]))\n",
        "    incidents = incidents.withColumn(\"month\", month(incidents[\"time_local\"]))\n",
        "    \n",
        "    filtered_incidents = incidents.filter((incidents.year == args.yearArg) & (incidents.month == args.monthArg))\n",
        "    \n",
        "    filtered_incidents.show()\n",
        "    \n",
        "    # counts.repartition(1).saveAsTextFile(\"\")\n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTV-wNskZHt4",
        "outputId": "509e62c3-a40c-4814-8f34-b2cf0a8feb5f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting month_incidents.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to test locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 month_incidents.py --monthArg 2 --yearArg 2017"
      ],
      "metadata": {
        "id": "rojSdKp3Zq32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In profs sample code they use plotly express, but: \n",
        "\n",
        "\n",
        "*   plotly express can only be used with pandas, which means we have to convert our spark df to a pandas df to use plotly express but obviously this is an issue with the size of our dataset\n",
        "*   to use regular plotly, we have to sign up/create an account to get an access key for their API calls to create the maps we need\n",
        "\n"
      ],
      "metadata": {
        "id": "FhHEKto2v28P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file road_density_incidents.py\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  \n",
        "  try:\n",
        "  \n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "nlvh4p56ZrQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 road_density_incidents.py"
      ],
      "metadata": {
        "id": "9G4EN0Z6bWDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file road_avg_response.py\n",
        "\n",
        "# 3. query the average response time for each road segment, and sort it to find\n",
        "#  which road segments have the highest and lowest response times. \n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from operator import add\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # replace this line with the s3 pass when testing over EMR\n",
        "  conf = SparkConf().setAppName('month_incidents').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "  spark = SparkSession(sc)\n",
        "\n",
        "  \n",
        "  try:\n",
        "\n",
        "    incidents = spark.read.parquet(\"incidents_10000.parquet\")\n",
        "\n",
        "    incidents = incidents.drop(\"latitude\", \"longitude\", \"emdCardNumber\", \"time_utc\", \"day_of_week\", \"weekend_or_not\", \"Incident_ID\", \"geometry\")\n",
        "\n",
        "    \n",
        "    incidents.show()\n",
        "  \n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "5zFAgEjObWog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d33dbf4a-3feb-43c8-e365-baf0d927942f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting road_avg_response.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 road_avg_response.py"
      ],
      "metadata": {
        "id": "PIOJwz7epRNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48498123-d7b9-43d1-cd43-f5a40cc8a21a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-05d8d868-e698-4ff0-b04b-1a86e0ffd750;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 803ms :: artifacts dl 36ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-05d8d868-e698-4ff0-b04b-1a86e0ffd750\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/13ms)\n",
            "23/04/27 00:37:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/27 00:37:44 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/27 00:37:44 INFO ResourceUtils: ==============================================================\n",
            "23/04/27 00:37:44 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/27 00:37:44 INFO ResourceUtils: ==============================================================\n",
            "23/04/27 00:37:44 INFO SparkContext: Submitted application: month_incidents\n",
            "23/04/27 00:37:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/27 00:37:44 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/27 00:37:44 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/27 00:37:44 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/27 00:37:44 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/27 00:37:44 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/27 00:37:44 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/27 00:37:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/27 00:37:44 INFO Utils: Successfully started service 'sparkDriver' on port 45041.\n",
            "23/04/27 00:37:45 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/27 00:37:45 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/27 00:37:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/27 00:37:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/27 00:37:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/27 00:37:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8baf7cdd-5aa2-4ece-992c-777fa5eefbdd\n",
            "23/04/27 00:37:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/27 00:37:45 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/27 00:37:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "23/04/27 00:37:46 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "23/04/27 00:37:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://df3c46de523d:4041\n",
            "23/04/27 00:37:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://df3c46de523d:45041/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://df3c46de523d:45041/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://df3c46de523d:45041/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://df3c46de523d:45041/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://df3c46de523d:45041/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://df3c46de523d:45041/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://df3c46de523d:45041/jars/com.101tec_zkclient-0.3.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://df3c46de523d:45041/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://df3c46de523d:45041/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://df3c46de523d:45041/jars/log4j_log4j-1.2.17.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://df3c46de523d:45041/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://df3c46de523d:45041/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/27 00:37:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/27 00:37:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/27 00:37:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/27 00:37:46 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/27 00:37:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/27 00:37:46 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/com.101tec_zkclient-0.3.jar\n",
            "23/04/27 00:37:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/27 00:37:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/27 00:37:46 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/log4j_log4j-1.2.17.jar\n",
            "23/04/27 00:37:46 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/27 00:37:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:46 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/27 00:37:47 INFO Executor: Starting executor ID driver on host df3c46de523d\n",
            "23/04/27 00:37:47 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/log4j_log4j-1.2.17.jar\n",
            "23/04/27 00:37:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/27 00:37:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/27 00:37:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/27 00:37:47 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/27 00:37:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/27 00:37:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/27 00:37:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/27 00:37:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/27 00:37:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/27 00:37:47 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/com.101tec_zkclient-0.3.jar\n",
            "23/04/27 00:37:47 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/27 00:37:47 INFO Executor: Fetching spark://df3c46de523d:45041/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO TransportClientFactory: Successfully created connection to df3c46de523d/172.28.0.12:45041 after 100 ms (0 ms spent in bootstraps)\n",
            "23/04/27 00:37:47 INFO Utils: Fetching spark://df3c46de523d:45041/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp272844184000750375.tmp\n",
            "23/04/27 00:37:47 INFO Utils: /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp272844184000750375.tmp has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/27 00:37:47 INFO Executor: Adding file:/tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/27 00:37:47 INFO Executor: Fetching spark://df3c46de523d:45041/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: Fetching spark://df3c46de523d:45041/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp2294615075221864383.tmp\n",
            "23/04/27 00:37:47 INFO Utils: /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp2294615075221864383.tmp has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/27 00:37:47 INFO Executor: Adding file:/tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/27 00:37:47 INFO Executor: Fetching spark://df3c46de523d:45041/jars/com.101tec_zkclient-0.3.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: Fetching spark://df3c46de523d:45041/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp186598171924474552.tmp\n",
            "23/04/27 00:37:47 INFO Utils: /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp186598171924474552.tmp has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/com.101tec_zkclient-0.3.jar\n",
            "23/04/27 00:37:47 INFO Executor: Adding file:/tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/27 00:37:47 INFO Executor: Fetching spark://df3c46de523d:45041/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: Fetching spark://df3c46de523d:45041/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp754627309410551287.tmp\n",
            "23/04/27 00:37:47 INFO Utils: /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp754627309410551287.tmp has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/27 00:37:47 INFO Executor: Adding file:/tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/27 00:37:47 INFO Executor: Fetching spark://df3c46de523d:45041/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: Fetching spark://df3c46de523d:45041/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp8331965049754116019.tmp\n",
            "23/04/27 00:37:47 INFO Utils: /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp8331965049754116019.tmp has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/27 00:37:47 INFO Executor: Adding file:/tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/27 00:37:47 INFO Executor: Fetching spark://df3c46de523d:45041/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:47 INFO Utils: Fetching spark://df3c46de523d:45041/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp5818940012238644280.tmp\n",
            "23/04/27 00:37:48 INFO Utils: /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp5818940012238644280.tmp has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/27 00:37:48 INFO Executor: Adding file:/tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/27 00:37:48 INFO Executor: Fetching spark://df3c46de523d:45041/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:48 INFO Utils: Fetching spark://df3c46de523d:45041/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp8669094487081169061.tmp\n",
            "23/04/27 00:37:48 INFO Utils: /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp8669094487081169061.tmp has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/27 00:37:48 INFO Executor: Adding file:/tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/27 00:37:48 INFO Executor: Fetching spark://df3c46de523d:45041/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:48 INFO Utils: Fetching spark://df3c46de523d:45041/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp399926915770262383.tmp\n",
            "23/04/27 00:37:48 INFO Utils: /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp399926915770262383.tmp has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/27 00:37:48 INFO Executor: Adding file:/tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/27 00:37:48 INFO Executor: Fetching spark://df3c46de523d:45041/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:48 INFO Utils: Fetching spark://df3c46de523d:45041/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp2198429938251252037.tmp\n",
            "23/04/27 00:37:48 INFO Utils: /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp2198429938251252037.tmp has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/27 00:37:48 INFO Executor: Adding file:/tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/27 00:37:48 INFO Executor: Fetching spark://df3c46de523d:45041/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:48 INFO Utils: Fetching spark://df3c46de523d:45041/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp8659636432989630653.tmp\n",
            "23/04/27 00:37:48 INFO Utils: /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp8659636432989630653.tmp has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/27 00:37:48 INFO Executor: Adding file:/tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/27 00:37:48 INFO Executor: Fetching spark://df3c46de523d:45041/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:48 INFO Utils: Fetching spark://df3c46de523d:45041/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp8035948697975715915.tmp\n",
            "23/04/27 00:37:48 INFO Utils: /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp8035948697975715915.tmp has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/27 00:37:48 INFO Executor: Adding file:/tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/27 00:37:48 INFO Executor: Fetching spark://df3c46de523d:45041/jars/log4j_log4j-1.2.17.jar with timestamp 1682555864133\n",
            "23/04/27 00:37:48 INFO Utils: Fetching spark://df3c46de523d:45041/jars/log4j_log4j-1.2.17.jar to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp4781023136428304454.tmp\n",
            "23/04/27 00:37:48 INFO Utils: /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/fetchFileTemp4781023136428304454.tmp has been previously copied to /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/log4j_log4j-1.2.17.jar\n",
            "23/04/27 00:37:48 INFO Executor: Adding file:/tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/userFiles-d82efdc1-75aa-47c1-9069-dee21d6eb951/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/27 00:37:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42969.\n",
            "23/04/27 00:37:48 INFO NettyBlockTransferService: Server created on df3c46de523d:42969\n",
            "23/04/27 00:37:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/27 00:37:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, df3c46de523d, 42969, None)\n",
            "23/04/27 00:37:48 INFO BlockManagerMasterEndpoint: Registering block manager df3c46de523d:42969 with 366.3 MiB RAM, BlockManagerId(driver, df3c46de523d, 42969, None)\n",
            "23/04/27 00:37:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, df3c46de523d, 42969, None)\n",
            "23/04/27 00:37:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, df3c46de523d, 42969, None)\n",
            "23/04/27 00:37:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/27 00:37:49 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/27 00:37:50 INFO InMemoryFileIndex: It took 55 ms to list leaf files for 1 paths.\n",
            "23/04/27 00:37:51 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/27 00:37:51 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/27 00:37:51 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/27 00:37:51 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/27 00:37:51 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/27 00:37:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/27 00:37:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 105.0 KiB, free 366.2 MiB)\n",
            "23/04/27 00:37:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/27 00:37:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on df3c46de523d:42969 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/27 00:37:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/27 00:37:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/27 00:37:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/27 00:37:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (df3c46de523d, executor driver, partition 0, PROCESS_LOCAL, 4653 bytes) taskResourceAssignments Map()\n",
            "23/04/27 00:37:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/27 00:37:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2406 bytes result sent to driver\n",
            "23/04/27 00:37:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1280 ms on df3c46de523d (executor driver) (1/1)\n",
            "23/04/27 00:37:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/27 00:37:53 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.661 s\n",
            "23/04/27 00:37:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/27 00:37:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/27 00:37:53 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.798535 s\n",
            "23/04/27 00:37:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on df3c46de523d:42969 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/27 00:37:56 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/27 00:37:56 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/27 00:37:57 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, time_local: timestamp, response_time_sec: double ... 6 more fields>\n",
            "23/04/27 00:37:57 INFO CodeGenerator: Code generated in 368.341264 ms\n",
            "23/04/27 00:37:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 354.4 KiB, free 366.0 MiB)\n",
            "23/04/27 00:37:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 365.9 MiB)\n",
            "23/04/27 00:37:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on df3c46de523d:42969 (size: 35.9 KiB, free: 366.3 MiB)\n",
            "23/04/27 00:37:57 INFO SparkContext: Created broadcast 1 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/27 00:37:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/27 00:37:57 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/27 00:37:57 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/27 00:37:57 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/27 00:37:57 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/27 00:37:57 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/27 00:37:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/27 00:37:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 18.8 KiB, free 365.9 MiB)\n",
            "23/04/27 00:37:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 365.9 MiB)\n",
            "23/04/27 00:37:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on df3c46de523d:42969 (size: 7.3 KiB, free: 366.3 MiB)\n",
            "23/04/27 00:37:58 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/27 00:37:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/27 00:37:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/27 00:37:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (df3c46de523d, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()\n",
            "23/04/27 00:37:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/27 00:37:58 INFO FileScanRDD: Reading File path: file:///content/incidents_10000.parquet/part-00000-f5680797-dbdc-4974-b7be-4ac232b10054-c000.snappy.parquet, range: 0-709306, partition values: [empty row]\n",
            "23/04/27 00:37:58 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/27 00:37:58 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3704 bytes result sent to driver\n",
            "23/04/27 00:37:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 913 ms on df3c46de523d (executor driver) (1/1)\n",
            "23/04/27 00:37:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/27 00:37:58 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.980 s\n",
            "23/04/27 00:37:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/27 00:37:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/04/27 00:37:58 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 1.026689 s\n",
            "23/04/27 00:37:59 INFO CodeGenerator: Code generated in 192.127278 ms\n",
            "+--------------------+-----------+------------+--------------------+-----------------+--------------------+--------------------+-------------+\n",
            "|         ID_Original|   latitude|   longitude|          time_local|response_time_sec|            geometry|         Dist_to_Seg|      XDSegID|\n",
            "+--------------------+-----------+------------+--------------------+-----------------+--------------------+--------------------+-------------+\n",
            "|ObjectId(59d3a819...|36.03722849|-86.78324314|2017-01-01 01:59:...|            268.0|POINT (-86.783243...|  13.550370382347305|1.524393684E9|\n",
            "|ObjectId(59d3a819...|36.03741402|-86.78657189| 2017-01-01 02:30:55|            512.0|POINT (-86.786571...|  3.6639521286614225|1.524356434E9|\n",
            "|ObjectId(59d3a81a...|36.21766267|-86.80869908| 2017-01-01 03:35:10|            271.0|POINT (-86.808699...|                null|         null|\n",
            "|ObjectId(59d3a81a...|36.05779026|-86.73972836|2017-01-01 04:29:...|            366.0|POINT (-86.739728...|                null|         null|\n",
            "|ObjectId(59d3a81a...|36.03922079|-86.60278517| 2017-01-01 05:09:13|            447.0|POINT (-86.602785...|   3.171781614079594| 4.49620819E8|\n",
            "|ObjectId(59d3a81a...|36.17626943|-86.77259127| 2017-01-01 05:19:06|            272.0|POINT (-86.772591...|  12.303987671832065|  3.9606837E8|\n",
            "|ObjectId(59d3a81b...|36.12457356|-86.70003704| 2017-01-01 05:58:05|            207.0|POINT (-86.700037...| 0.43977600410940326| 1.56041062E8|\n",
            "|ObjectId(59d3a81b...|36.03722849|-86.78324314| 2017-01-01 07:24:15|            442.0|POINT (-86.783243...|  13.550370382347305|1.524393684E9|\n",
            "|ObjectId(59d3a81b...|36.16627491|-86.61177092| 2017-01-01 07:29:12|            715.0|POINT (-86.611770...|  3.5960680523120807|1.524450692E9|\n",
            "|ObjectId(59d3a81b...|36.12177807|-86.76291116| 2017-01-01 08:07:49|            320.0|POINT (-86.762911...|   1.162080398654493|1.524630585E9|\n",
            "|ObjectId(59d3a81b...|36.27212796|-86.68956725| 2017-01-01 08:38:23|            268.0|POINT (-86.689567...|   5.619154783749885| 4.50433846E8|\n",
            "|ObjectId(59d3a81c...|36.17676163| -86.7726931| 2017-01-01 10:11:37|            608.0|POINT (-86.772693...|   2.369052960154209|1.524470172E9|\n",
            "|ObjectId(59d3a81c...|36.04522787|-86.71480047| 2017-01-01 11:05:26|            185.0|POINT (-86.714800...|  1.0270451796578366| 4.49631045E8|\n",
            "|ObjectId(59d3a81c...|36.13823241|-86.80714448|2017-01-01 11:45:...|            272.0|POINT (-86.807144...|   0.818697334180834| 4.49629705E8|\n",
            "|ObjectId(59d3a81c...|36.12235632|-86.78650828| 2017-01-01 11:48:17|            246.0|POINT (-86.786508...| 0.12196591114412407| 4.49629776E8|\n",
            "|ObjectId(59d3a81c...|36.03722849|-86.78324314| 2017-01-01 12:52:46|            483.0|POINT (-86.783243...|  13.550370382347305|1.524393684E9|\n",
            "|ObjectId(59d3a81d...|36.21678674| -86.6235188| 2017-01-01 13:12:07|            430.0|POINT (-86.623518...|0.001559666505496297| 4.49620803E8|\n",
            "|ObjectId(59d3a81d...|36.16102973|-86.77720388| 2017-01-01 16:00:59|            158.0|POINT (-86.777203...|  0.6504744043237719| 4.49630784E8|\n",
            "|ObjectId(59d3a81e...|36.04537322|-86.65885054|2017-01-01 17:07:...|            374.0|POINT (-86.658850...|  1.9612656553367578| 1.52431367E9|\n",
            "|ObjectId(59d3a81e...|36.15876015| -86.6027096| 2017-01-01 18:50:41|            417.0|POINT (-86.602709...|                null|         null|\n",
            "+--------------------+-----------+------------+--------------------+-----------------+--------------------+--------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "23/04/27 00:37:59 INFO SparkUI: Stopped Spark web UI at http://df3c46de523d:4041\n",
            "23/04/27 00:37:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/27 00:37:59 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/27 00:37:59 INFO BlockManager: BlockManager stopped\n",
            "23/04/27 00:37:59 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/27 00:37:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/27 00:37:59 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/27 00:38:00 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/27 00:38:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-d99d4dfe-620a-4139-8f00-057f0085ccb6\n",
            "23/04/27 00:38:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07/pyspark-7bec82a4-f3e6-44a4-a0a9-7027870d5523\n",
            "23/04/27 00:38:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-44908ae2-3493-470a-9c77-f6e13b297e07\n"
          ]
        }
      ]
    }
  ]
}